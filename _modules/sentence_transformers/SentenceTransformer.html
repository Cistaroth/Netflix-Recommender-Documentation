
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sentence_transformers.SentenceTransformer &#8212; Netflix Recommender 0.1.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=c9d161cb" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=01f34227"></script>
    <script src="../../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/sentence_transformers/SentenceTransformer';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Netflix Recommender 0.1.0 documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api/modules.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api/modules.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">sentence_transformers.SentenceTransformer</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for sentence_transformers.SentenceTransformer</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">importlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">inspect</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">queue</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">Iterator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">multiprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Queue</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">overload</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy.typing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">npt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">HfApi</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">packaging</span><span class="w"> </span><span class="kn">import</span> <span class="n">version</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm.autonotebook</span><span class="w"> </span><span class="kn">import</span> <span class="n">trange</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PreTrainedModel</span><span class="p">,</span> <span class="n">is_torch_npu_available</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.dynamic_module_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_class_from_dynamic_module</span><span class="p">,</span> <span class="n">get_relative_import_files</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">deprecated</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers.model_card</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformerModelCardData</span><span class="p">,</span> <span class="n">generate_model_card</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Router</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers.models.Module</span><span class="w"> </span><span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers.similarity_functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimilarityFunction</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">__MODEL_HUB_ORGANIZATION__</span><span class="p">,</span> <span class="n">__version__</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.evaluation</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceEvaluator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.fit_mixin</span><span class="w"> </span><span class="kn">import</span> <span class="n">FitMixin</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pooling</span><span class="p">,</span> <span class="n">Transformer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.peft_mixin</span><span class="w"> </span><span class="kn">import</span> <span class="n">PeftAdapterMixin</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">quantize_embeddings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.util</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">batch_to_device</span><span class="p">,</span>
    <span class="n">get_device_name</span><span class="p">,</span>
    <span class="n">import_from_string</span><span class="p">,</span>
    <span class="n">is_sentence_transformer_model</span><span class="p">,</span>
    <span class="n">load_dir_path</span><span class="p">,</span>
    <span class="n">load_file_path</span><span class="p">,</span>
    <span class="n">save_to_hub_args_decorator</span><span class="p">,</span>
    <span class="n">truncate_embeddings</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="SentenceTransformer">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SentenceTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">,</span> <span class="n">FitMixin</span><span class="p">,</span> <span class="n">PeftAdapterMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loads or creates a SentenceTransformer model that can be used to map sentences / text to embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_name_or_path (str, optional): If it is a filepath on disk, it loads the model from that path. If it is not a path,</span>
<span class="sd">            it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model</span>
<span class="sd">            from the Hugging Face Hub with that name.</span>
<span class="sd">        modules (Iterable[nn.Module], optional): A list of torch Modules that should be called sequentially, can be used to create custom</span>
<span class="sd">            SentenceTransformer models from scratch.</span>
<span class="sd">        device (str, optional): Device (like &quot;cuda&quot;, &quot;cpu&quot;, &quot;mps&quot;, &quot;npu&quot;) that should be used for computation. If None, checks if a GPU</span>
<span class="sd">            can be used.</span>
<span class="sd">        prompts (Dict[str, str], optional): A dictionary with prompts for the model. The key is the prompt name, the value is the prompt text.</span>
<span class="sd">            The prompt text will be prepended before any text to encode. For example:</span>
<span class="sd">            `{&quot;query&quot;: &quot;query: &quot;, &quot;passage&quot;: &quot;passage: &quot;}` or `{&quot;clustering&quot;: &quot;Identify the main category based on the</span>
<span class="sd">            titles in &quot;}`.</span>
<span class="sd">        default_prompt_name (str, optional): The name of the prompt that should be used by default. If not set,</span>
<span class="sd">            no prompt will be applied.</span>
<span class="sd">        similarity_fn_name (str or SimilarityFunction, optional): The name of the similarity function to use. Valid options are &quot;cosine&quot;, &quot;dot&quot;,</span>
<span class="sd">            &quot;euclidean&quot;, and &quot;manhattan&quot;. If not set, it is automatically set to &quot;cosine&quot; if `similarity` or</span>
<span class="sd">            `similarity_pairwise` are called while `model.similarity_fn_name` is still `None`.</span>
<span class="sd">        cache_folder (str, optional): Path to store models. Can also be set by the SENTENCE_TRANSFORMERS_HOME environment variable.</span>
<span class="sd">        trust_remote_code (bool, optional): Whether or not to allow for custom models defined on the Hub in their own modeling files.</span>
<span class="sd">            This option should only be set to True for repositories you trust and in which you have read the code, as it</span>
<span class="sd">            will execute code present on the Hub on your local machine.</span>
<span class="sd">        revision (str, optional): The specific model version to use. It can be a branch name, a tag name, or a commit id,</span>
<span class="sd">            for a stored model on Hugging Face.</span>
<span class="sd">        local_files_only (bool, optional): Whether or not to only look at local files (i.e., do not try to download the model).</span>
<span class="sd">        token (bool or str, optional): Hugging Face authentication token to download private models.</span>
<span class="sd">        use_auth_token (bool or str, optional): Deprecated argument. Please use `token` instead.</span>
<span class="sd">        truncate_dim (int, optional): The dimension to truncate sentence embeddings to. Defaults to None.</span>
<span class="sd">        model_kwargs (Dict[str, Any], optional): Additional model configuration parameters to be passed to the Hugging Face Transformers model.</span>
<span class="sd">            Particularly useful options are:</span>

<span class="sd">            - ``torch_dtype``: Override the default `torch.dtype` and load the model under a specific `dtype`.</span>
<span class="sd">              The different options are:</span>

<span class="sd">                    1. ``torch.float16``, ``torch.bfloat16`` or ``torch.float``: load in a specified</span>
<span class="sd">                    ``dtype``, ignoring the model&#39;s ``config.torch_dtype`` if one exists. If not specified - the model will</span>
<span class="sd">                    get loaded in ``torch.float`` (fp32).</span>

<span class="sd">                    2. ``&quot;auto&quot;`` - A ``torch_dtype`` entry in the ``config.json`` file of the model will be</span>
<span class="sd">                    attempted to be used. If this entry isn&#39;t found then next check the ``dtype`` of the first weight in</span>
<span class="sd">                    the checkpoint that&#39;s of a floating point type and use that as ``dtype``. This will load the model</span>
<span class="sd">                    using the ``dtype`` it was saved in at the end of the training. It can&#39;t be used as an indicator of how</span>
<span class="sd">                    the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.</span>
<span class="sd">            - ``attn_implementation``: The attention implementation to use in the model (if relevant). Can be any of</span>
<span class="sd">              `&quot;eager&quot;` (manual implementation of the attention), `&quot;sdpa&quot;` (using `F.scaled_dot_product_attention</span>
<span class="sd">              &lt;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html&gt;`_),</span>
<span class="sd">              or `&quot;flash_attention_2&quot;` (using `Dao-AILab/flash-attention &lt;https://github.com/Dao-AILab/flash-attention&gt;`_).</span>
<span class="sd">              By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual `&quot;eager&quot;`</span>
<span class="sd">              implementation.</span>
<span class="sd">            - ``provider``: If backend is &quot;onnx&quot;, this is the provider to use for inference, for example &quot;CPUExecutionProvider&quot;,</span>
<span class="sd">              &quot;CUDAExecutionProvider&quot;, etc. See https://onnxruntime.ai/docs/execution-providers/ for all ONNX execution providers.</span>
<span class="sd">            - ``file_name``: If backend is &quot;onnx&quot; or &quot;openvino&quot;, this is the file name to load, useful for loading optimized</span>
<span class="sd">              or quantized ONNX or OpenVINO models.</span>
<span class="sd">            - ``export``: If backend is &quot;onnx&quot; or &quot;openvino&quot;, then this is a boolean flag specifying whether this model should</span>
<span class="sd">              be exported to the backend. If not specified, the model will be exported only if the model repository or directory</span>
<span class="sd">              does not already contain an exported model.</span>

<span class="sd">            See the `PreTrainedModel.from_pretrained</span>
<span class="sd">            &lt;https://huggingface.co/docs/transformers/en/main_classes/model#transformers.PreTrainedModel.from_pretrained&gt;`_</span>
<span class="sd">            documentation for more details.</span>
<span class="sd">        tokenizer_kwargs (Dict[str, Any], optional): Additional tokenizer configuration parameters to be passed to the Hugging Face Transformers tokenizer.</span>
<span class="sd">            See the `AutoTokenizer.from_pretrained</span>
<span class="sd">            &lt;https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained&gt;`_</span>
<span class="sd">            documentation for more details.</span>
<span class="sd">        config_kwargs (Dict[str, Any], optional): Additional model configuration parameters to be passed to the Hugging Face Transformers config.</span>
<span class="sd">            See the `AutoConfig.from_pretrained</span>
<span class="sd">            &lt;https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoConfig.from_pretrained&gt;`_</span>
<span class="sd">            documentation for more details.</span>
<span class="sd">        model_card_data (:class:`~sentence_transformers.model_card.SentenceTransformerModelCardData`, optional): A model</span>
<span class="sd">            card data object that contains information about the model. This is used to generate a model card when saving</span>
<span class="sd">            the model. If not set, a default model card data object is created.</span>
<span class="sd">        backend (str): The backend to use for inference. Can be one of &quot;torch&quot; (default), &quot;onnx&quot;, or &quot;openvino&quot;.</span>
<span class="sd">            See https://sbert.net/docs/sentence_transformer/usage/efficiency.html for benchmarking information</span>
<span class="sd">            on the different backends.</span>

<span class="sd">    Example:</span>
<span class="sd">        ::</span>

<span class="sd">            from sentence_transformers import SentenceTransformer</span>

<span class="sd">            # Load a pre-trained SentenceTransformer model</span>
<span class="sd">            model = SentenceTransformer(&#39;all-mpnet-base-v2&#39;)</span>

<span class="sd">            # Encode some texts</span>
<span class="sd">            sentences = [</span>
<span class="sd">                &quot;The weather is lovely today.&quot;,</span>
<span class="sd">                &quot;It&#39;s so sunny outside!&quot;,</span>
<span class="sd">                &quot;He drove to the stadium.&quot;,</span>
<span class="sd">            ]</span>
<span class="sd">            embeddings = model.encode(sentences)</span>
<span class="sd">            print(embeddings.shape)</span>
<span class="sd">            # (3, 768)</span>

<span class="sd">            # Get the similarity scores between all sentences</span>
<span class="sd">            similarities = model.similarity(embeddings, embeddings)</span>
<span class="sd">            print(similarities)</span>
<span class="sd">            # tensor([[1.0000, 0.6817, 0.0492],</span>
<span class="sd">            #         [0.6817, 1.0000, 0.0421],</span>
<span class="sd">            #         [0.0492, 0.0421, 1.0000]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model_card_data_class</span> <span class="o">=</span> <span class="n">SentenceTransformerModelCardData</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_name_or_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">default_prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">similarity_fn_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">SimilarityFunction</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trust_remote_code</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_auth_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">config_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_card_data</span><span class="p">:</span> <span class="n">SentenceTransformerModelCardData</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="s2">&quot;onnx&quot;</span><span class="p">,</span> <span class="s2">&quot;openvino&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Note: self._load_sbert_model can also update `self.prompts` and `self.default_prompt_name`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;document&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">prompts</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span> <span class="o">=</span> <span class="n">default_prompt_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">similarity_fn_name</span> <span class="o">=</span> <span class="n">similarity_fn_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trust_remote_code</span> <span class="o">=</span> <span class="n">trust_remote_code</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">truncate_dim</span> <span class="o">=</span> <span class="n">truncate_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span> <span class="o">=</span> <span class="n">model_card_data</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data_class</span><span class="p">(</span><span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_card_vars</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_card_text</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_length_mapping</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span>
        <span class="k">if</span> <span class="n">use_auth_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`token` and `use_auth_token` are both specified. Please set only the argument `token`.&quot;</span>
                <span class="p">)</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">use_auth_token</span>

        <span class="k">if</span> <span class="n">cache_folder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_folder</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;SENTENCE_TRANSFORMERS_HOME&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">get_device_name</span><span class="p">()</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Use pytorch device_name: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;hpu&quot;</span> <span class="ow">and</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;optimum&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">optimum.habana.transformers.modeling_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">adapt_transformers_to_gaudi</span>

            <span class="n">adapt_transformers_to_gaudi</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">model_name_or_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">model_name_or_path</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Load pretrained </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">model_name_or_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># Old models that don&#39;t belong to any organization</span>
            <span class="n">basic_transformer_models</span> <span class="o">=</span> <span class="p">[</span>
                <span class="s2">&quot;albert-base-v1&quot;</span><span class="p">,</span>
                <span class="s2">&quot;albert-base-v2&quot;</span><span class="p">,</span>
                <span class="s2">&quot;albert-large-v1&quot;</span><span class="p">,</span>
                <span class="s2">&quot;albert-large-v2&quot;</span><span class="p">,</span>
                <span class="s2">&quot;albert-xlarge-v1&quot;</span><span class="p">,</span>
                <span class="s2">&quot;albert-xlarge-v2&quot;</span><span class="p">,</span>
                <span class="s2">&quot;albert-xxlarge-v1&quot;</span><span class="p">,</span>
                <span class="s2">&quot;albert-xxlarge-v2&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-cased-finetuned-mrpc&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-chinese&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-german-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-german-dbmdz-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-german-dbmdz-uncased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-multilingual-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-multilingual-uncased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-large-cased-whole-word-masking-finetuned-squad&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-large-cased-whole-word-masking&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-large-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-large-uncased-whole-word-masking-finetuned-squad&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-large-uncased-whole-word-masking&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bert-large-uncased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;camembert-base&quot;</span><span class="p">,</span>
                <span class="s2">&quot;ctrl&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilbert-base-cased-distilled-squad&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilbert-base-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilbert-base-german-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilbert-base-multilingual-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilbert-base-uncased-distilled-squad&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilgpt2&quot;</span><span class="p">,</span>
                <span class="s2">&quot;distilroberta-base&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt2-large&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt2-medium&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt2-xl&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt2&quot;</span><span class="p">,</span>
                <span class="s2">&quot;openai-gpt&quot;</span><span class="p">,</span>
                <span class="s2">&quot;roberta-base-openai-detector&quot;</span><span class="p">,</span>
                <span class="s2">&quot;roberta-base&quot;</span><span class="p">,</span>
                <span class="s2">&quot;roberta-large-mnli&quot;</span><span class="p">,</span>
                <span class="s2">&quot;roberta-large-openai-detector&quot;</span><span class="p">,</span>
                <span class="s2">&quot;roberta-large&quot;</span><span class="p">,</span>
                <span class="s2">&quot;t5-11b&quot;</span><span class="p">,</span>
                <span class="s2">&quot;t5-3b&quot;</span><span class="p">,</span>
                <span class="s2">&quot;t5-base&quot;</span><span class="p">,</span>
                <span class="s2">&quot;t5-large&quot;</span><span class="p">,</span>
                <span class="s2">&quot;t5-small&quot;</span><span class="p">,</span>
                <span class="s2">&quot;transfo-xl-wt103&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-clm-ende-1024&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-clm-enfr-1024&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-mlm-100-1280&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-mlm-17-1280&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-mlm-en-2048&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-mlm-ende-1024&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-mlm-enfr-1024&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-mlm-enro-1024&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-mlm-tlm-xnli15-1024&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-mlm-xnli15-1024&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-roberta-base&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-roberta-large-finetuned-conll02-dutch&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-roberta-large-finetuned-conll02-spanish&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-roberta-large-finetuned-conll03-english&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-roberta-large-finetuned-conll03-german&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlm-roberta-large&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlnet-base-cased&quot;</span><span class="p">,</span>
                <span class="s2">&quot;xlnet-large-cased&quot;</span><span class="p">,</span>
            <span class="p">]</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">):</span>
                <span class="c1"># Not a path, load from hub</span>
                <span class="k">if</span> <span class="s2">&quot;</span><span class="se">\\</span><span class="s2">&quot;</span> <span class="ow">in</span> <span class="n">model_name_or_path</span> <span class="ow">or</span> <span class="n">model_name_or_path</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Path </span><span class="si">{</span><span class="n">model_name_or_path</span><span class="si">}</span><span class="s2"> not found&quot;</span><span class="p">)</span>

                <span class="k">if</span> <span class="s2">&quot;/&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_name_or_path</span> <span class="ow">and</span> <span class="n">model_name_or_path</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">basic_transformer_models</span><span class="p">:</span>
                    <span class="c1"># A model from sentence-transformers</span>
                    <span class="n">model_name_or_path</span> <span class="o">=</span> <span class="n">__MODEL_HUB_ORGANIZATION__</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">model_name_or_path</span>
            <span class="n">has_modules</span> <span class="o">=</span> <span class="n">is_sentence_transformer_model</span><span class="p">(</span>
                <span class="n">model_name_or_path</span><span class="p">,</span>
                <span class="n">token</span><span class="p">,</span>
                <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">has_modules</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_model_type</span><span class="p">(</span>
                    <span class="n">model_name_or_path</span><span class="p">,</span>
                    <span class="n">token</span><span class="p">,</span>
                    <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="n">modules</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_sbert_model</span><span class="p">(</span>
                    <span class="n">model_name_or_path</span><span class="p">,</span>
                    <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                    <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">trust_remote_code</span><span class="o">=</span><span class="n">trust_remote_code</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
                    <span class="n">tokenizer_kwargs</span><span class="o">=</span><span class="n">tokenizer_kwargs</span><span class="p">,</span>
                    <span class="n">config_kwargs</span><span class="o">=</span><span class="n">config_kwargs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">modules</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_auto_model</span><span class="p">(</span>
                    <span class="n">model_name_or_path</span><span class="p">,</span>
                    <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                    <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">trust_remote_code</span><span class="o">=</span><span class="n">trust_remote_code</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
                    <span class="n">tokenizer_kwargs</span><span class="o">=</span><span class="n">tokenizer_kwargs</span><span class="p">,</span>
                    <span class="n">config_kwargs</span><span class="o">=</span><span class="n">config_kwargs</span><span class="p">,</span>
                    <span class="n">has_modules</span><span class="o">=</span><span class="n">has_modules</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">modules</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">OrderedDict</span><span class="p">):</span>
            <span class="n">modules</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span> <span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">modules</span><span class="p">)])</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>

        <span class="c1"># Ensure all tensors in the model are of the same dtype as the first tensor</span>
        <span class="c1"># This is necessary if the first module has been given a lower precision via</span>
        <span class="c1"># model_kwargs[&quot;torch_dtype&quot;]. The rest of the model should be loaded in the same dtype</span>
        <span class="c1"># See #2887 for more details</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">dtype</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_hpu_graph_enabled</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Default prompt name &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span><span class="si">}</span><span class="s2">&#39; not found in the configured prompts &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;dictionary with keys </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">!r}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="ow">and</span> <span class="p">(</span><span class="n">non_empty_keys</span> <span class="o">:=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">]):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">non_empty_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;1 prompt is loaded, with the key: </span><span class="si">{</span><span class="n">non_empty_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">non_empty_keys</span><span class="p">)</span><span class="si">}</span><span class="s2"> prompts are loaded, with the keys: </span><span class="si">{</span><span class="n">non_empty_keys</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Default prompt name is set to &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span><span class="si">}</span><span class="s2">&#39;. &quot;</span>
                <span class="s2">&quot;This prompt will be applied to all `encode()` calls, except if `encode()` &quot;</span>
                <span class="s2">&quot;is called with `prompt` or `prompt_name` parameters.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Ideally, INSTRUCTOR models should set `include_prompt=False` in their pooling configuration, but</span>
        <span class="c1"># that would be a breaking change for users currently using the InstructorEmbedding project.</span>
        <span class="c1"># So, instead we hardcode setting it for the main INSTRUCTOR models, and otherwise give a warning if we</span>
        <span class="c1"># suspect the user is using an INSTRUCTOR model.</span>
        <span class="k">if</span> <span class="n">model_name_or_path</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hkunlp/instructor-base&quot;</span><span class="p">,</span> <span class="s2">&quot;hkunlp/instructor-large&quot;</span><span class="p">,</span> <span class="s2">&quot;hkunlp/instructor-xl&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_pooling_include_prompt</span><span class="p">(</span><span class="n">include_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">model_name_or_path</span>
            <span class="ow">and</span> <span class="s2">&quot;/&quot;</span> <span class="ow">in</span> <span class="n">model_name_or_path</span>
            <span class="ow">and</span> <span class="s2">&quot;instructor&quot;</span> <span class="ow">in</span> <span class="n">model_name_or_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="n">module</span><span class="o">.</span><span class="n">include_prompt</span> <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Pooling</span><span class="p">)]):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Instructor models require `include_prompt=False` in the pooling configuration. &quot;</span>
                    <span class="s2">&quot;Either update the model configuration or call `model.set_pooling_include_prompt(False)` after loading the model.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Pass the model to the model card data for later use in generating a model card upon saving this model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="SentenceTransformer.get_backend">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.get_backend">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_backend</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="s2">&quot;onnx&quot;</span><span class="p">,</span> <span class="s2">&quot;openvino&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the backend used for inference, which can be one of &quot;torch&quot;, &quot;onnx&quot;, or &quot;openvino&quot;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The backend used for inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">backend</span></div>


<div class="viewcode-block" id="SentenceTransformer.get_model_kwargs">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.get_model_kwargs">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_model_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the keyword arguments specific to this model for the `encode`, `encode_query`, or `encode_document` methods.</span>

<span class="sd">        Example:</span>

<span class="sd">            &gt;&gt;&gt; from sentence_transformers import SentenceTransformer, SparseEncoder</span>
<span class="sd">            &gt;&gt;&gt; SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;).get_model_kwargs()</span>
<span class="sd">            []</span>
<span class="sd">            &gt;&gt;&gt; SentenceTransformer(&quot;jinaai/jina-embeddings-v4&quot;, trust_remote_code=True).get_model_kwargs()</span>
<span class="sd">            [&#39;task&#39;, &#39;truncate_dim&#39;]</span>
<span class="sd">            &gt;&gt;&gt; SparseEncoder(&quot;opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill&quot;).get_model_kwargs()</span>
<span class="sd">            [&#39;task&#39;]</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[str]: A list of keyword arguments for the forward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">())</span>
        <span class="n">forward_kwargs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">modules</span><span class="p">:</span>
            <span class="n">module_name</span><span class="p">,</span> <span class="n">module</span> <span class="o">=</span> <span class="n">modules</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Router</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">route_modules</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">sub_modules</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">modules</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">route_modules</span><span class="o">.</span><span class="n">named_children</span><span class="p">()))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span> <span class="ow">and</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span><span class="p">:</span>
                <span class="n">forward_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span><span class="p">[</span><span class="n">module_name</span><span class="p">])</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;forward_kwargs&quot;</span><span class="p">):</span>
                <span class="n">forward_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">forward_kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">forward_kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="SentenceTransformer.encode_query">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.encode_query">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode_query</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span> <span class="s2">&quot;token_embeddings&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes sentence embeddings specifically optimized for query representation.</span>

<span class="sd">        This method is a specialized version of :meth:`encode` that differs in exactly two ways:</span>

<span class="sd">        1. If no ``prompt_name`` or ``prompt`` is provided, it uses a predefined &quot;query&quot; prompt,</span>
<span class="sd">           if available in the model&#39;s ``prompts`` dictionary.</span>
<span class="sd">        2. It sets the ``task`` to &quot;query&quot;. If the model has a :class:`~sentence_transformers.models.Router`</span>
<span class="sd">           module, it will use the &quot;query&quot; task type to route the input through the appropriate submodules.</span>

<span class="sd">        .. tip::</span>

<span class="sd">            If you are unsure whether you should use :meth:`encode`, :meth:`encode_query`, or :meth:`encode_document`,</span>
<span class="sd">            your best bet is to use :meth:`encode_query` and :meth:`encode_document` for Information Retrieval tasks</span>
<span class="sd">            with clear query and document/passage distinction, and use :meth:`encode` for all other tasks.</span>

<span class="sd">            Note that :meth:`encode` is the most general method and can be used for any task, including Information</span>
<span class="sd">            Retrieval, and that if the model was not trained with predefined prompts and/or task types, then all three</span>
<span class="sd">            methods will return identical embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            sentences (Union[str, List[str]]): The sentences to embed.</span>
<span class="sd">            prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the `prompts` dictionary,</span>
<span class="sd">                which is either set in the constructor or loaded from the model configuration. For example if</span>
<span class="sd">                ``prompt_name`` is &quot;query&quot; and the ``prompts`` is {&quot;query&quot;: &quot;query: &quot;, ...}, then the sentence &quot;What</span>
<span class="sd">                is the capital of France?&quot; will be encoded as &quot;query: What is the capital of France?&quot; because the sentence</span>
<span class="sd">                is appended to the prompt. If ``prompt`` is also set, this argument is ignored. Defaults to None.</span>
<span class="sd">            prompt (Optional[str], optional): The prompt to use for encoding. For example, if the prompt is &quot;query: &quot;, then the</span>
<span class="sd">                sentence &quot;What is the capital of France?&quot; will be encoded as &quot;query: What is the capital of France?&quot;</span>
<span class="sd">                because the sentence is appended to the prompt. If ``prompt`` is set, ``prompt_name`` is ignored. Defaults to None.</span>
<span class="sd">            batch_size (int, optional): The batch size used for the computation. Defaults to 32.</span>
<span class="sd">            show_progress_bar (bool, optional): Whether to output a progress bar when encode sentences. Defaults to None.</span>
<span class="sd">            output_value (Optional[Literal[&quot;sentence_embedding&quot;, &quot;token_embeddings&quot;]], optional): The type of embeddings to return:</span>
<span class="sd">                &quot;sentence_embedding&quot; to get sentence embeddings, &quot;token_embeddings&quot; to get wordpiece token embeddings, and `None`,</span>
<span class="sd">                to get all output values. Defaults to &quot;sentence_embedding&quot;.</span>
<span class="sd">            precision (Literal[&quot;float32&quot;, &quot;int8&quot;, &quot;uint8&quot;, &quot;binary&quot;, &quot;ubinary&quot;], optional): The precision to use for the embeddings.</span>
<span class="sd">                Can be &quot;float32&quot;, &quot;int8&quot;, &quot;uint8&quot;, &quot;binary&quot;, or &quot;ubinary&quot;. All non-float32 precisions are quantized embeddings.</span>
<span class="sd">                Quantized embeddings are smaller in size and faster to compute, but may have a lower accuracy. They are useful for</span>
<span class="sd">                reducing the size of the embeddings of a corpus for semantic search, among other tasks. Defaults to &quot;float32&quot;.</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether the output should be a list of numpy vectors. If False, it is a list of PyTorch tensors.</span>
<span class="sd">                Defaults to True.</span>
<span class="sd">            convert_to_tensor (bool, optional): Whether the output should be one large tensor. Overwrites `convert_to_numpy`.</span>
<span class="sd">                Defaults to False.</span>
<span class="sd">            device (Union[str, List[str], None], optional): Device(s) to use for computation. Can be:</span>

<span class="sd">                - A single device string (e.g., &quot;cuda:0&quot;, &quot;cpu&quot;) for single-process encoding</span>
<span class="sd">                - A list of device strings (e.g., [&quot;cuda:0&quot;, &quot;cuda:1&quot;], [&quot;cpu&quot;, &quot;cpu&quot;, &quot;cpu&quot;, &quot;cpu&quot;]) to distribute</span>
<span class="sd">                  encoding across multiple processes</span>
<span class="sd">                - None to auto-detect available device for single-process encoding</span>
<span class="sd">                If a list is provided, multi-process encoding will be used. Defaults to None.</span>
<span class="sd">            normalize_embeddings (bool, optional): Whether to normalize returned vectors to have length 1. In that case,</span>
<span class="sd">                the faster dot-product (util.dot_score) instead of cosine similarity can be used. Defaults to False.</span>
<span class="sd">            truncate_dim (int, optional): The dimension to truncate sentence embeddings to.</span>
<span class="sd">                Truncation is especially interesting for `Matryoshka models &lt;https://sbert.net/examples/sentence_transformer/training/matryoshka/README.html&gt;`_,</span>
<span class="sd">                i.e. models that are trained to still produce useful embeddings even if the embedding dimension is reduced.</span>
<span class="sd">                Truncated embeddings require less memory and are faster to perform retrieval with, but note that inference</span>
<span class="sd">                is just as fast, and the embedding performance is worse than the full embeddings. If None, the ``truncate_dim``</span>
<span class="sd">                from the model initialization is used. Defaults to None.</span>
<span class="sd">            pool (Dict[Literal[&quot;input&quot;, &quot;output&quot;, &quot;processes&quot;], Any], optional): A pool created by `start_multi_process_pool()`</span>
<span class="sd">                for multi-process encoding. If provided, the encoding will be distributed across multiple processes.</span>
<span class="sd">                This is recommended for large datasets and when multiple GPUs are available. Defaults to None.</span>
<span class="sd">            chunk_size (int, optional): Size of chunks for multi-process encoding. Only used with multiprocessing, i.e. when</span>
<span class="sd">                ``pool`` is not None or ``device`` is a list. If None, a sensible default is calculated. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[List[Tensor], ndarray, Tensor]: By default, a 2d numpy array with shape [num_inputs, output_dimension] is returned.</span>
<span class="sd">            If only one string input is provided, then the output is a 1d array with shape [output_dimension]. If ``convert_to_tensor``,</span>
<span class="sd">            a torch Tensor is returned instead. If ``self.truncate_dim &lt;= output_dimension`` then output_dimension is ``self.truncate_dim``.</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                from sentence_transformers import SentenceTransformer</span>

<span class="sd">                # Load a pre-trained SentenceTransformer model</span>
<span class="sd">                model = SentenceTransformer(&quot;mixedbread-ai/mxbai-embed-large-v1&quot;)</span>

<span class="sd">                # Encode some queries</span>
<span class="sd">                queries = [</span>
<span class="sd">                    &quot;What are the effects of climate change?&quot;,</span>
<span class="sd">                    &quot;History of artificial intelligence&quot;,</span>
<span class="sd">                    &quot;Technical specifications product XYZ&quot;,</span>
<span class="sd">                ]</span>

<span class="sd">                # Using query-specific encoding</span>
<span class="sd">                embeddings = model.encode_query(queries)</span>
<span class="sd">                print(embeddings.shape)</span>
<span class="sd">                # (3, 768)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prompt_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;query&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="ow">and</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prompt_name</span> <span class="o">=</span> <span class="s2">&quot;query&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
            <span class="n">sentences</span><span class="o">=</span><span class="n">sentences</span><span class="p">,</span>
            <span class="n">prompt_name</span><span class="o">=</span><span class="n">prompt_name</span><span class="p">,</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bar</span><span class="o">=</span><span class="n">show_progress_bar</span><span class="p">,</span>
            <span class="n">output_value</span><span class="o">=</span><span class="n">output_value</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span><span class="p">,</span>
            <span class="n">convert_to_tensor</span><span class="o">=</span><span class="n">convert_to_tensor</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">normalize_embeddings</span><span class="o">=</span><span class="n">normalize_embeddings</span><span class="p">,</span>
            <span class="n">truncate_dim</span><span class="o">=</span><span class="n">truncate_dim</span><span class="p">,</span>
            <span class="n">pool</span><span class="o">=</span><span class="n">pool</span><span class="p">,</span>
            <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span>
            <span class="n">task</span><span class="o">=</span><span class="s2">&quot;query&quot;</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="SentenceTransformer.encode_document">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.encode_document">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode_document</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span> <span class="s2">&quot;token_embeddings&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes sentence embeddings specifically optimized for document/passage representation.</span>

<span class="sd">        This method is a specialized version of :meth:`encode` that differs in exactly two ways:</span>

<span class="sd">        1. If no ``prompt_name`` or ``prompt`` is provided, it uses a predefined &quot;document&quot; prompt,</span>
<span class="sd">           if available in the model&#39;s ``prompts`` dictionary.</span>
<span class="sd">        2. It sets the ``task`` to &quot;document&quot;. If the model has a :class:`~sentence_transformers.models.Router`</span>
<span class="sd">           module, it will use the &quot;document&quot; task type to route the input through the appropriate submodules.</span>

<span class="sd">        .. tip::</span>

<span class="sd">            If you are unsure whether you should use :meth:`encode`, :meth:`encode_query`, or :meth:`encode_document`,</span>
<span class="sd">            your best bet is to use :meth:`encode_query` and :meth:`encode_document` for Information Retrieval tasks</span>
<span class="sd">            with clear query and document/passage distinction, and use :meth:`encode` for all other tasks.</span>

<span class="sd">            Note that :meth:`encode` is the most general method and can be used for any task, including Information</span>
<span class="sd">            Retrieval, and that if the model was not trained with predefined prompts and/or task types, then all three</span>
<span class="sd">            methods will return identical embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            sentences (Union[str, List[str]]): The sentences to embed.</span>
<span class="sd">            prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the `prompts` dictionary,</span>
<span class="sd">                which is either set in the constructor or loaded from the model configuration. For example if</span>
<span class="sd">                ``prompt_name`` is &quot;query&quot; and the ``prompts`` is {&quot;query&quot;: &quot;query: &quot;, ...}, then the sentence &quot;What</span>
<span class="sd">                is the capital of France?&quot; will be encoded as &quot;query: What is the capital of France?&quot; because the sentence</span>
<span class="sd">                is appended to the prompt. If ``prompt`` is also set, this argument is ignored. Defaults to None.</span>
<span class="sd">            prompt (Optional[str], optional): The prompt to use for encoding. For example, if the prompt is &quot;query: &quot;, then the</span>
<span class="sd">                sentence &quot;What is the capital of France?&quot; will be encoded as &quot;query: What is the capital of France?&quot;</span>
<span class="sd">                because the sentence is appended to the prompt. If ``prompt`` is set, ``prompt_name`` is ignored. Defaults to None.</span>
<span class="sd">            batch_size (int, optional): The batch size used for the computation. Defaults to 32.</span>
<span class="sd">            show_progress_bar (bool, optional): Whether to output a progress bar when encode sentences. Defaults to None.</span>
<span class="sd">            output_value (Optional[Literal[&quot;sentence_embedding&quot;, &quot;token_embeddings&quot;]], optional): The type of embeddings to return:</span>
<span class="sd">                &quot;sentence_embedding&quot; to get sentence embeddings, &quot;token_embeddings&quot; to get wordpiece token embeddings, and `None`,</span>
<span class="sd">                to get all output values. Defaults to &quot;sentence_embedding&quot;.</span>
<span class="sd">            precision (Literal[&quot;float32&quot;, &quot;int8&quot;, &quot;uint8&quot;, &quot;binary&quot;, &quot;ubinary&quot;], optional): The precision to use for the embeddings.</span>
<span class="sd">                Can be &quot;float32&quot;, &quot;int8&quot;, &quot;uint8&quot;, &quot;binary&quot;, or &quot;ubinary&quot;. All non-float32 precisions are quantized embeddings.</span>
<span class="sd">                Quantized embeddings are smaller in size and faster to compute, but may have a lower accuracy. They are useful for</span>
<span class="sd">                reducing the size of the embeddings of a corpus for semantic search, among other tasks. Defaults to &quot;float32&quot;.</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether the output should be a list of numpy vectors. If False, it is a list of PyTorch tensors.</span>
<span class="sd">                Defaults to True.</span>
<span class="sd">            convert_to_tensor (bool, optional): Whether the output should be one large tensor. Overwrites `convert_to_numpy`.</span>
<span class="sd">                Defaults to False.</span>
<span class="sd">            device (Union[str, List[str], None], optional): Device(s) to use for computation. Can be:</span>

<span class="sd">                - A single device string (e.g., &quot;cuda:0&quot;, &quot;cpu&quot;) for single-process encoding</span>
<span class="sd">                - A list of device strings (e.g., [&quot;cuda:0&quot;, &quot;cuda:1&quot;], [&quot;cpu&quot;, &quot;cpu&quot;, &quot;cpu&quot;, &quot;cpu&quot;]) to distribute</span>
<span class="sd">                  encoding across multiple processes</span>
<span class="sd">                - None to auto-detect available device for single-process encoding</span>
<span class="sd">                If a list is provided, multi-process encoding will be used. Defaults to None.</span>
<span class="sd">            normalize_embeddings (bool, optional): Whether to normalize returned vectors to have length 1. In that case,</span>
<span class="sd">                the faster dot-product (util.dot_score) instead of cosine similarity can be used. Defaults to False.</span>
<span class="sd">            truncate_dim (int, optional): The dimension to truncate sentence embeddings to.</span>
<span class="sd">                Truncation is especially interesting for `Matryoshka models &lt;https://sbert.net/examples/sentence_transformer/training/matryoshka/README.html&gt;`_,</span>
<span class="sd">                i.e. models that are trained to still produce useful embeddings even if the embedding dimension is reduced.</span>
<span class="sd">                Truncated embeddings require less memory and are faster to perform retrieval with, but note that inference</span>
<span class="sd">                is just as fast, and the embedding performance is worse than the full embeddings. If None, the ``truncate_dim``</span>
<span class="sd">                from the model initialization is used. Defaults to None.</span>
<span class="sd">            pool (Dict[Literal[&quot;input&quot;, &quot;output&quot;, &quot;processes&quot;], Any], optional): A pool created by `start_multi_process_pool()`</span>
<span class="sd">                for multi-process encoding. If provided, the encoding will be distributed across multiple processes.</span>
<span class="sd">                This is recommended for large datasets and when multiple GPUs are available. Defaults to None.</span>
<span class="sd">            chunk_size (int, optional): Size of chunks for multi-process encoding. Only used with multiprocessing, i.e. when</span>
<span class="sd">                ``pool`` is not None or ``device`` is a list. If None, a sensible default is calculated. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[List[Tensor], ndarray, Tensor]: By default, a 2d numpy array with shape [num_inputs, output_dimension] is returned.</span>
<span class="sd">            If only one string input is provided, then the output is a 1d array with shape [output_dimension]. If ``convert_to_tensor``,</span>
<span class="sd">            a torch Tensor is returned instead. If ``self.truncate_dim &lt;= output_dimension`` then output_dimension is ``self.truncate_dim``.</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                from sentence_transformers import SentenceTransformer</span>

<span class="sd">                # Load a pre-trained SentenceTransformer model</span>
<span class="sd">                model = SentenceTransformer(&quot;mixedbread-ai/mxbai-embed-large-v1&quot;)</span>

<span class="sd">                # Encode some documents</span>
<span class="sd">                documents = [</span>
<span class="sd">                    &quot;This research paper discusses the effects of climate change on marine life.&quot;,</span>
<span class="sd">                    &quot;The article explores the history of artificial intelligence development.&quot;,</span>
<span class="sd">                    &quot;This document contains technical specifications for the new product line.&quot;,</span>
<span class="sd">                ]</span>

<span class="sd">                # Using document-specific encoding</span>
<span class="sd">                embeddings = model.encode_document(documents)</span>
<span class="sd">                print(embeddings.shape)</span>
<span class="sd">                # (3, 768)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prompt_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">candidate_prompt_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;document&quot;</span><span class="p">,</span> <span class="s2">&quot;passage&quot;</span><span class="p">,</span> <span class="s2">&quot;corpus&quot;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">candidate_prompt_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="p">:</span>
                    <span class="n">prompt_name</span> <span class="o">=</span> <span class="n">candidate_prompt_name</span>
                    <span class="k">break</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
            <span class="n">sentences</span><span class="o">=</span><span class="n">sentences</span><span class="p">,</span>
            <span class="n">prompt_name</span><span class="o">=</span><span class="n">prompt_name</span><span class="p">,</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bar</span><span class="o">=</span><span class="n">show_progress_bar</span><span class="p">,</span>
            <span class="n">output_value</span><span class="o">=</span><span class="n">output_value</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span><span class="p">,</span>
            <span class="n">convert_to_tensor</span><span class="o">=</span><span class="n">convert_to_tensor</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">normalize_embeddings</span><span class="o">=</span><span class="n">normalize_embeddings</span><span class="p">,</span>
            <span class="n">truncate_dim</span><span class="o">=</span><span class="n">truncate_dim</span><span class="p">,</span>
            <span class="n">pool</span><span class="o">=</span><span class="n">pool</span><span class="p">,</span>
            <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span>
            <span class="n">task</span><span class="o">=</span><span class="s2">&quot;document&quot;</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>


    <span class="c1"># Return a single tensor because we&#39;re passing a single sentence.</span>
    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span> <span class="s2">&quot;token_embeddings&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> <span class="o">...</span>

    <span class="c1"># Return a single array, because convert_to_numpy is True</span>
    <span class="c1"># and &quot;sentence_embeddings&quot; is passed</span>
    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span> <span class="o">...</span>

    <span class="c1"># Return a single tensor, because convert_to_tensor is True</span>
    <span class="c1"># and &quot;sentence_embeddings&quot; is passed</span>
    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> <span class="o">...</span>

    <span class="c1"># Return a list of tensors. Value of convert_ doesn&#39;t matter.</span>
    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span> <span class="s2">&quot;token_embeddings&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span> <span class="o">...</span>

    <span class="c1"># Return a list of dict of features, ignore the conversion args.</span>
    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span> <span class="o">...</span>

    <span class="c1"># Return a dict of features, ignore the conversion args.</span>
    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span> <span class="o">...</span>

    <span class="c1"># If &quot;token_embeddings&quot; is True, then the output is a single tensor.</span>
    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;token_embeddings&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> <span class="o">...</span>

<div class="viewcode-block" id="SentenceTransformer.encode">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.encode">[docs]</a>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span> <span class="s2">&quot;token_embeddings&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_to_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes sentence embeddings.</span>

<span class="sd">        .. tip::</span>

<span class="sd">            If you are unsure whether you should use :meth:`encode`, :meth:`encode_query`, or :meth:`encode_document`,</span>
<span class="sd">            your best bet is to use :meth:`encode_query` and :meth:`encode_document` for Information Retrieval tasks</span>
<span class="sd">            with clear query and document/passage distinction, and use :meth:`encode` for all other tasks.</span>

<span class="sd">            Note that :meth:`encode` is the most general method and can be used for any task, including Information</span>
<span class="sd">            Retrieval, and that if the model was not trained with predefined prompts and/or task types, then all three</span>
<span class="sd">            methods will return identical embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            sentences (Union[str, List[str]]): The sentences to embed.</span>
<span class="sd">            prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the `prompts` dictionary,</span>
<span class="sd">                which is either set in the constructor or loaded from the model configuration. For example if</span>
<span class="sd">                ``prompt_name`` is &quot;query&quot; and the ``prompts`` is {&quot;query&quot;: &quot;query: &quot;, ...}, then the sentence &quot;What</span>
<span class="sd">                is the capital of France?&quot; will be encoded as &quot;query: What is the capital of France?&quot; because the sentence</span>
<span class="sd">                is appended to the prompt. If ``prompt`` is also set, this argument is ignored. Defaults to None.</span>
<span class="sd">            prompt (Optional[str], optional): The prompt to use for encoding. For example, if the prompt is &quot;query: &quot;, then the</span>
<span class="sd">                sentence &quot;What is the capital of France?&quot; will be encoded as &quot;query: What is the capital of France?&quot;</span>
<span class="sd">                because the sentence is appended to the prompt. If ``prompt`` is set, ``prompt_name`` is ignored. Defaults to None.</span>
<span class="sd">            batch_size (int, optional): The batch size used for the computation. Defaults to 32.</span>
<span class="sd">            show_progress_bar (bool, optional): Whether to output a progress bar when encode sentences. Defaults to None.</span>
<span class="sd">            output_value (Optional[Literal[&quot;sentence_embedding&quot;, &quot;token_embeddings&quot;]], optional): The type of embeddings to return:</span>
<span class="sd">                &quot;sentence_embedding&quot; to get sentence embeddings, &quot;token_embeddings&quot; to get wordpiece token embeddings, and `None`,</span>
<span class="sd">                to get all output values. Defaults to &quot;sentence_embedding&quot;.</span>
<span class="sd">            precision (Literal[&quot;float32&quot;, &quot;int8&quot;, &quot;uint8&quot;, &quot;binary&quot;, &quot;ubinary&quot;], optional): The precision to use for the embeddings.</span>
<span class="sd">                Can be &quot;float32&quot;, &quot;int8&quot;, &quot;uint8&quot;, &quot;binary&quot;, or &quot;ubinary&quot;. All non-float32 precisions are quantized embeddings.</span>
<span class="sd">                Quantized embeddings are smaller in size and faster to compute, but may have a lower accuracy. They are useful for</span>
<span class="sd">                reducing the size of the embeddings of a corpus for semantic search, among other tasks. Defaults to &quot;float32&quot;.</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether the output should be a list of numpy vectors. If False, it is a list of PyTorch tensors.</span>
<span class="sd">                Defaults to True.</span>
<span class="sd">            convert_to_tensor (bool, optional): Whether the output should be one large tensor. Overwrites `convert_to_numpy`.</span>
<span class="sd">                Defaults to False.</span>
<span class="sd">            device (Union[str, List[str], None], optional): Device(s) to use for computation. Can be:</span>

<span class="sd">                - A single device string (e.g., &quot;cuda:0&quot;, &quot;cpu&quot;) for single-process encoding</span>
<span class="sd">                - A list of device strings (e.g., [&quot;cuda:0&quot;, &quot;cuda:1&quot;], [&quot;cpu&quot;, &quot;cpu&quot;, &quot;cpu&quot;, &quot;cpu&quot;]) to distribute</span>
<span class="sd">                  encoding across multiple processes</span>
<span class="sd">                - None to auto-detect available device for single-process encoding</span>
<span class="sd">                If a list is provided, multi-process encoding will be used. Defaults to None.</span>
<span class="sd">            normalize_embeddings (bool, optional): Whether to normalize returned vectors to have length 1. In that case,</span>
<span class="sd">                the faster dot-product (util.dot_score) instead of cosine similarity can be used. Defaults to False.</span>
<span class="sd">            truncate_dim (int, optional): The dimension to truncate sentence embeddings to.</span>
<span class="sd">                Truncation is especially interesting for `Matryoshka models &lt;https://sbert.net/examples/sentence_transformer/training/matryoshka/README.html&gt;`_,</span>
<span class="sd">                i.e. models that are trained to still produce useful embeddings even if the embedding dimension is reduced.</span>
<span class="sd">                Truncated embeddings require less memory and are faster to perform retrieval with, but note that inference</span>
<span class="sd">                is just as fast, and the embedding performance is worse than the full embeddings. If None, the ``truncate_dim``</span>
<span class="sd">                from the model initialization is used. Defaults to None.</span>
<span class="sd">            pool (Dict[Literal[&quot;input&quot;, &quot;output&quot;, &quot;processes&quot;], Any], optional): A pool created by `start_multi_process_pool()`</span>
<span class="sd">                for multi-process encoding. If provided, the encoding will be distributed across multiple processes.</span>
<span class="sd">                This is recommended for large datasets and when multiple GPUs are available. Defaults to None.</span>
<span class="sd">            chunk_size (int, optional): Size of chunks for multi-process encoding. Only used with multiprocessing, i.e. when</span>
<span class="sd">                ``pool`` is not None or ``device`` is a list. If None, a sensible default is calculated. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[List[Tensor], ndarray, Tensor]: By default, a 2d numpy array with shape [num_inputs, output_dimension] is returned.</span>
<span class="sd">            If only one string input is provided, then the output is a 1d array with shape [output_dimension]. If ``convert_to_tensor``,</span>
<span class="sd">            a torch Tensor is returned instead. If ``self.truncate_dim &lt;= output_dimension`` then output_dimension is ``self.truncate_dim``.</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                from sentence_transformers import SentenceTransformer</span>

<span class="sd">                # Load a pre-trained SentenceTransformer model</span>
<span class="sd">                model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)</span>

<span class="sd">                # Encode some texts</span>
<span class="sd">                sentences = [</span>
<span class="sd">                    &quot;The weather is lovely today.&quot;,</span>
<span class="sd">                    &quot;It&#39;s so sunny outside!&quot;,</span>
<span class="sd">                    &quot;He drove to the stadium.&quot;,</span>
<span class="sd">                ]</span>
<span class="sd">                embeddings = model.encode(sentences)</span>
<span class="sd">                print(embeddings.shape)</span>
<span class="sd">                # (3, 768)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;hpu&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_hpu_graph_enabled</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">habana_frameworks.torch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ht</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ht</span><span class="p">,</span> <span class="s2">&quot;hpu&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ht</span><span class="o">.</span><span class="n">hpu</span><span class="p">,</span> <span class="s2">&quot;wrap_in_hpu_graph&quot;</span><span class="p">):</span>
                <span class="n">ht</span><span class="o">.</span><span class="n">hpu</span><span class="o">.</span><span class="n">wrap_in_hpu_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">disable_tensor_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_hpu_graph_enabled</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">show_progress_bar</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">show_progress_bar</span> <span class="o">=</span> <span class="n">logger</span><span class="o">.</span><span class="n">getEffectiveLevel</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">convert_to_tensor</span><span class="p">:</span>
            <span class="n">convert_to_numpy</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">output_value</span> <span class="o">!=</span> <span class="s2">&quot;sentence_embedding&quot;</span><span class="p">:</span>
            <span class="n">convert_to_tensor</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">convert_to_numpy</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Cast an individual input to a list with length 1</span>
        <span class="n">input_was_string</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="s2">&quot;__len__&quot;</span><span class="p">):</span>
            <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentences</span><span class="p">]</span>
            <span class="n">input_was_string</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Throw an error if unused kwargs are passed, except &#39;task&#39; which is always allowed, even</span>
        <span class="c1"># when it does not do anything (as e.g. there&#39;s no Router module in the model)</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_model_kwargs</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">unused_kwargs</span> <span class="o">:=</span> <span class="nb">set</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">)</span> <span class="o">-</span> <span class="p">{</span><span class="s2">&quot;task&quot;</span><span class="p">}:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.encode() has been called with additional keyword arguments that this model does not use: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">unused_kwargs</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="o">+</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;As per </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.get_model_kwargs(), the valid additional keyword arguments are: </span><span class="si">{</span><span class="n">model_kwargs</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="k">if</span> <span class="n">model_kwargs</span>
                    <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;As per </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.get_model_kwargs(), this model does not accept any additional keyword arguments.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># If pool or a list of devices is provided, use multi-process encoding</span>
        <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_multi_process</span><span class="p">(</span>
                <span class="n">sentences</span><span class="p">,</span>
                <span class="c1"># Utility and post-processing parameters</span>
                <span class="n">show_progress_bar</span><span class="o">=</span><span class="n">show_progress_bar</span><span class="p">,</span>
                <span class="n">input_was_string</span><span class="o">=</span><span class="n">input_was_string</span><span class="p">,</span>
                <span class="c1"># Multi-process encoding parameters</span>
                <span class="n">pool</span><span class="o">=</span><span class="n">pool</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span>
                <span class="c1"># Encoding parameters</span>
                <span class="n">prompt_name</span><span class="o">=</span><span class="n">prompt_name</span><span class="p">,</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">output_value</span><span class="o">=</span><span class="n">output_value</span><span class="p">,</span>
                <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
                <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span><span class="p">,</span>
                <span class="n">convert_to_tensor</span><span class="o">=</span><span class="n">convert_to_tensor</span><span class="p">,</span>
                <span class="n">normalize_embeddings</span><span class="o">=</span><span class="n">normalize_embeddings</span><span class="p">,</span>
                <span class="n">truncate_dim</span><span class="o">=</span><span class="n">truncate_dim</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Original encoding logic when not using multi-process</span>
        <span class="n">allowed_precisions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">precision</span> <span class="ow">and</span> <span class="n">precision</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_precisions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision </span><span class="si">{</span><span class="n">precision</span><span class="si">!r}</span><span class="s2"> is not supported&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">prompt_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="p">[</span><span class="n">prompt_name</span><span class="p">]</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Prompt name &#39;</span><span class="si">{</span><span class="n">prompt_name</span><span class="si">}</span><span class="s2">&#39; not found in the configured prompts dictionary with keys </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">!r}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">prompt_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Encode with either a `prompt`, a `prompt_name`, or neither, but not both. &quot;</span>
                    <span class="s2">&quot;Ignoring the `prompt_name` in favor of `prompt`.&quot;</span>
                <span class="p">)</span>

        <span class="n">extra_features</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">sentence</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>

            <span class="c1"># Some models (e.g. INSTRUCTOR, GRIT) require removing the prompt before pooling</span>
            <span class="c1"># Tracking the prompt length allow us to remove the prompt during pooling</span>
            <span class="n">length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_prompt_length</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">extra_features</span><span class="p">[</span><span class="s2">&quot;prompt_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">length</span>

        <span class="c1"># Here, device is either a single device string (e.g., &quot;cuda:0&quot;, &quot;cpu&quot;) for single-process encoding or None</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">truncate_dim</span> <span class="o">=</span> <span class="n">truncate_dim</span> <span class="k">if</span> <span class="n">truncate_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_dim</span>

        <span class="n">all_embeddings</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">length_sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">([</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_text_length</span><span class="p">(</span><span class="n">sen</span><span class="p">)</span> <span class="k">for</span> <span class="n">sen</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">])</span>
        <span class="n">sentences_sorted</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentences</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">length_sorted_idx</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">start_index</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Batches&quot;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bar</span><span class="p">):</span>
            <span class="n">sentences_batch</span> <span class="o">=</span> <span class="n">sentences_sorted</span><span class="p">[</span><span class="n">start_index</span> <span class="p">:</span> <span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sentences_batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;hpu&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;input_ids&quot;</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
                    <span class="n">curr_tokenize_len</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
                    <span class="n">additional_pad_len</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">curr_tokenize_len</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="o">-</span> <span class="n">curr_tokenize_len</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">features</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="n">features</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">curr_tokenize_len</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">additional_pad_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">),</span>
                        <span class="p">),</span>
                        <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">features</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="n">features</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">curr_tokenize_len</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">additional_pad_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">),</span>
                        <span class="p">),</span>
                        <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
                        <span class="n">features</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                            <span class="p">(</span>
                                <span class="n">features</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">],</span>
                                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">curr_tokenize_len</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">additional_pad_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">),</span>
                            <span class="p">),</span>
                            <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                        <span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="n">batch_to_device</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
            <span class="n">features</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">extra_features</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">out_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;hpu&quot;</span><span class="p">:</span>
                    <span class="n">out_features</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">out_features</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">truncate_dim</span><span class="p">:</span>
                    <span class="n">out_features</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">truncate_embeddings</span><span class="p">(</span>
                        <span class="n">out_features</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">],</span> <span class="n">truncate_dim</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">output_value</span> <span class="o">==</span> <span class="s2">&quot;token_embeddings&quot;</span><span class="p">:</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">token_emb</span><span class="p">,</span> <span class="n">attention</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">out_features</span><span class="p">[</span><span class="n">output_value</span><span class="p">],</span> <span class="n">out_features</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]):</span>
                        <span class="n">last_mask_id</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                        <span class="k">while</span> <span class="n">last_mask_id</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">attention</span><span class="p">[</span><span class="n">last_mask_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">last_mask_id</span> <span class="o">-=</span> <span class="mi">1</span>

                        <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_emb</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="n">last_mask_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="k">elif</span> <span class="n">output_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># Return all outputs</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">out_features</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">])):</span>
                        <span class="n">batch_item</span> <span class="o">=</span> <span class="p">{}</span>
                        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">out_features</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                            <span class="k">try</span><span class="p">:</span>
                                <span class="n">batch_item</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                            <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
                                <span class="c1"># Handle non-indexable values (like prompt_length)</span>
                                <span class="n">batch_item</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
                        <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_item</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>  <span class="c1"># Sentence embeddings</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">out_features</span><span class="p">[</span><span class="n">output_value</span><span class="p">]</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">normalize_embeddings</span><span class="p">:</span>
                        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

                    <span class="c1"># fixes for #522 and #487 to avoid oom problems on gpu with large datasets</span>
                    <span class="k">if</span> <span class="n">convert_to_numpy</span><span class="p">:</span>
                        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

                <span class="n">all_embeddings</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

        <span class="n">all_embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">all_embeddings</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">length_sorted_idx</span><span class="p">)]</span>

        <span class="k">if</span> <span class="n">all_embeddings</span> <span class="ow">and</span> <span class="n">precision</span> <span class="ow">and</span> <span class="n">precision</span> <span class="o">!=</span> <span class="s2">&quot;float32&quot;</span><span class="p">:</span>
            <span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">quantize_embeddings</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">convert_to_tensor</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                    <span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">convert_to_numpy</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">all_embeddings</span> <span class="ow">and</span> <span class="n">all_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span>
                    <span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">emb</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">emb</span> <span class="ow">in</span> <span class="n">all_embeddings</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">emb</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">emb</span> <span class="ow">in</span> <span class="n">all_embeddings</span><span class="p">])</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">all_embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span> <span class="k">for</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="n">all_embeddings</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">input_was_string</span><span class="p">:</span>
            <span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">all_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">all_embeddings</span></div>


<div class="viewcode-block" id="SentenceTransformer.forward">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="n">module_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Router</span><span class="p">):</span>
                <span class="n">module_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">module_kwarg_keys</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">module_kwarg_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="p">[])</span>
                <span class="n">module_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">key</span><span class="p">:</span> <span class="n">value</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">module_kwarg_keys</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;forward_kwargs&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">forward_kwargs</span><span class="p">)</span>
                <span class="p">}</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">module_kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">input</span></div>


    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">similarity_fn_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span> <span class="s2">&quot;dot&quot;</span><span class="p">,</span> <span class="s2">&quot;euclidean&quot;</span><span class="p">,</span> <span class="s2">&quot;manhattan&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the name of the similarity function used by :meth:`SentenceTransformer.similarity` and :meth:`SentenceTransformer.similarity_pairwise`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Optional[str]: The name of the similarity function. Can be None if not set, in which case it will</span>
<span class="sd">                default to &quot;cosine&quot; when first called.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; model = SentenceTransformer(&quot;multi-qa-mpnet-base-dot-v1&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model.similarity_fn_name</span>
<span class="sd">            &#39;dot&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_similarity_fn_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">similarity_fn_name</span> <span class="o">=</span> <span class="n">SimilarityFunction</span><span class="o">.</span><span class="n">COSINE</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_similarity_fn_name</span>

    <span class="nd">@similarity_fn_name</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">similarity_fn_name</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span> <span class="s2">&quot;dot&quot;</span><span class="p">,</span> <span class="s2">&quot;euclidean&quot;</span><span class="p">,</span> <span class="s2">&quot;manhattan&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="n">SimilarityFunction</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">SimilarityFunction</span><span class="p">):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_similarity_fn_name</span> <span class="o">=</span> <span class="n">value</span>

        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_similarity</span> <span class="o">=</span> <span class="n">SimilarityFunction</span><span class="o">.</span><span class="n">to_similarity_fn</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_similarity_pairwise</span> <span class="o">=</span> <span class="n">SimilarityFunction</span><span class="o">.</span><span class="n">to_similarity_pairwise_fn</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">embeddings2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings1</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">embeddings2</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> <span class="o">...</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]],</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the similarity between two collections of embeddings. The output will be a matrix with the similarity</span>
<span class="sd">        scores between all embeddings from the first parameter and all embeddings from the second parameter. This</span>
<span class="sd">        differs from `similarity_pairwise` which computes the similarity between each pair of embeddings.</span>
<span class="sd">        This method supports only embeddings with fp32 precision and does not accommodate quantized embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            embeddings1 (Union[Tensor, ndarray]): [num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</span>
<span class="sd">            embeddings2 (Union[Tensor, ndarray]): [num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                &gt;&gt;&gt; model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)</span>
<span class="sd">                &gt;&gt;&gt; sentences = [</span>
<span class="sd">                ...     &quot;The weather is so nice!&quot;,</span>
<span class="sd">                ...     &quot;It&#39;s so sunny outside.&quot;,</span>
<span class="sd">                ...     &quot;He&#39;s driving to the movie theater.&quot;,</span>
<span class="sd">                ...     &quot;She&#39;s going to the cinema.&quot;,</span>
<span class="sd">                ... ]</span>
<span class="sd">                &gt;&gt;&gt; embeddings = model.encode(sentences, normalize_embeddings=True)</span>
<span class="sd">                &gt;&gt;&gt; model.similarity(embeddings, embeddings)</span>
<span class="sd">                tensor([[1.0000, 0.7235, 0.0290, 0.1309],</span>
<span class="sd">                        [0.7235, 1.0000, 0.0613, 0.1129],</span>
<span class="sd">                        [0.0290, 0.0613, 1.0000, 0.5027],</span>
<span class="sd">                        [0.1309, 0.1129, 0.5027, 1.0000]])</span>
<span class="sd">                &gt;&gt;&gt; model.similarity_fn_name</span>
<span class="sd">                &quot;cosine&quot;</span>
<span class="sd">                &gt;&gt;&gt; model.similarity_fn_name = &quot;euclidean&quot;</span>
<span class="sd">                &gt;&gt;&gt; model.similarity(embeddings, embeddings)</span>
<span class="sd">                tensor([[-0.0000, -0.7437, -1.3935, -1.3184],</span>
<span class="sd">                        [-0.7437, -0.0000, -1.3702, -1.3320],</span>
<span class="sd">                        [-1.3935, -1.3702, -0.0000, -0.9973],</span>
<span class="sd">                        [-1.3184, -1.3320, -0.9973, -0.0000]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity_fn_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">similarity_fn_name</span> <span class="o">=</span> <span class="n">SimilarityFunction</span><span class="o">.</span><span class="n">COSINE</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_similarity</span>

    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">similarity_pairwise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">embeddings2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">similarity_pairwise</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">embeddings1</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">embeddings2</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> <span class="o">...</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">similarity_pairwise</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]],</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the similarity between two collections of embeddings. The output will be a vector with the similarity</span>
<span class="sd">        scores between each pair of embeddings.</span>
<span class="sd">        This method supports only embeddings with fp32 precision and does not accommodate quantized embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            embeddings1 (Union[Tensor, ndarray]): [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</span>
<span class="sd">            embeddings2 (Union[Tensor, ndarray]): [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: A [num_embeddings]-shaped torch tensor with pairwise similarity scores.</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                &gt;&gt;&gt; model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)</span>
<span class="sd">                &gt;&gt;&gt; sentences = [</span>
<span class="sd">                ...     &quot;The weather is so nice!&quot;,</span>
<span class="sd">                ...     &quot;It&#39;s so sunny outside.&quot;,</span>
<span class="sd">                ...     &quot;He&#39;s driving to the movie theater.&quot;,</span>
<span class="sd">                ...     &quot;She&#39;s going to the cinema.&quot;,</span>
<span class="sd">                ... ]</span>
<span class="sd">                &gt;&gt;&gt; embeddings = model.encode(sentences, normalize_embeddings=True)</span>
<span class="sd">                &gt;&gt;&gt; model.similarity_pairwise(embeddings[::2], embeddings[1::2])</span>
<span class="sd">                tensor([0.7235, 0.5027])</span>
<span class="sd">                &gt;&gt;&gt; model.similarity_fn_name</span>
<span class="sd">                &quot;cosine&quot;</span>
<span class="sd">                &gt;&gt;&gt; model.similarity_fn_name = &quot;euclidean&quot;</span>
<span class="sd">                &gt;&gt;&gt; model.similarity_pairwise(embeddings[::2], embeddings[1::2])</span>
<span class="sd">                tensor([-0.7437, -0.9973])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity_fn_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">similarity_fn_name</span> <span class="o">=</span> <span class="n">SimilarityFunction</span><span class="o">.</span><span class="n">COSINE</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_similarity_pairwise</span>

<div class="viewcode-block" id="SentenceTransformer.start_multi_process_pool">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.start_multi_process_pool">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">start_multi_process_pool</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">target_devices</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Starts a multi-process pool to process the encoding with several independent processes</span>
<span class="sd">        via :meth:`SentenceTransformer.encode_multi_process &lt;sentence_transformers.SentenceTransformer.encode_multi_process&gt;`.</span>

<span class="sd">        This method is recommended if you want to encode on multiple GPUs or CPUs. It is advised</span>
<span class="sd">        to start only one process per GPU. This method works together with encode_multi_process</span>
<span class="sd">        and stop_multi_process_pool.</span>

<span class="sd">        Args:</span>
<span class="sd">            target_devices (List[str], optional): PyTorch target devices, e.g. [&quot;cuda:0&quot;, &quot;cuda:1&quot;, ...],</span>
<span class="sd">                [&quot;npu:0&quot;, &quot;npu:1&quot;, ...], or [&quot;cpu&quot;, &quot;cpu&quot;, &quot;cpu&quot;, &quot;cpu&quot;]. If target_devices is None and CUDA/NPU</span>
<span class="sd">                is available, then all available CUDA/NPU devices will be used. If target_devices is None and</span>
<span class="sd">                CUDA/NPU is not available, then 4 CPU devices will be used.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: A dictionary with the target processes, an input queue, and an output queue.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">target_devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">target_devices</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())]</span>
            <span class="k">elif</span> <span class="n">is_torch_npu_available</span><span class="p">():</span>
                <span class="n">target_devices</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;npu:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">npu</span><span class="o">.</span><span class="n">device_count</span><span class="p">())]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;CUDA/NPU is not available. Starting 4 CPU workers&quot;</span><span class="p">)</span>
                <span class="n">target_devices</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Start multi-process pool on devices: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">target_devices</span><span class="p">))))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">share_memory</span><span class="p">()</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;spawn&quot;</span><span class="p">)</span>
        <span class="n">input_queue</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
        <span class="n">output_queue</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
        <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">device_id</span> <span class="ow">in</span> <span class="n">target_devices</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
                <span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">_encode_multi_process_worker</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">device_id</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">input_queue</span><span class="p">,</span> <span class="n">output_queue</span><span class="p">),</span>
                <span class="n">daemon</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">input_queue</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="n">output_queue</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">:</span> <span class="n">processes</span><span class="p">}</span></div>


<div class="viewcode-block" id="SentenceTransformer.stop_multi_process_pool">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.stop_multi_process_pool">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">stop_multi_process_pool</span><span class="p">(</span><span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Stops all processes started with start_multi_process_pool.</span>

<span class="sd">        Args:</span>
<span class="sd">            pool (Dict[str, object]): A dictionary containing the input queue, output queue, and process list.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pool</span><span class="p">[</span><span class="s2">&quot;processes&quot;</span><span class="p">]:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pool</span><span class="p">[</span><span class="s2">&quot;processes&quot;</span><span class="p">]:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
            <span class="n">p</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="n">pool</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="n">pool</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></div>


<div class="viewcode-block" id="SentenceTransformer.encode_multi_process">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.encode_multi_process">[docs]</a>
    <span class="nd">@deprecated</span><span class="p">(</span>
        <span class="s2">&quot;The `encode_multi_process` method has been deprecated, and its functionality has been integrated into `encode`. &quot;</span>
        <span class="s2">&quot;You can now call `encode` with the same parameters to achieve multi-process encoding.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode_multi_process</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sentences</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prompt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="s2">&quot;uint8&quot;</span><span class="p">,</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;ubinary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span><span class="p">,</span>
        <span class="n">normalize_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        .. warning::</span>
<span class="sd">            This method is deprecated. You can now call :meth:`SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;`</span>
<span class="sd">            with the same parameters instead, which will automatically handle multi-process encoding using the provided ``pool``.</span>

<span class="sd">        Encodes a list of sentences using multiple processes and GPUs via</span>
<span class="sd">        :meth:`SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;`.</span>
<span class="sd">        The sentences are chunked into smaller packages and sent to individual processes, which encode them on different</span>
<span class="sd">        GPUs or CPUs. This method is only suitable for encoding large sets of sentences.</span>

<span class="sd">        Args:</span>
<span class="sd">            sentences (List[str]): List of sentences to encode.</span>
<span class="sd">            pool (Dict[Literal[&quot;input&quot;, &quot;output&quot;, &quot;processes&quot;], Any]): A pool of workers started with</span>
<span class="sd">                :meth:`SentenceTransformer.start_multi_process_pool &lt;sentence_transformers.SentenceTransformer.start_multi_process_pool&gt;`.</span>
<span class="sd">            prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the `prompts` dictionary,</span>
<span class="sd">                which is either set in the constructor or loaded from the model configuration. For example if</span>
<span class="sd">                ``prompt_name`` is &quot;query&quot; and the ``prompts`` is {&quot;query&quot;: &quot;query: &quot;, ...}, then the sentence &quot;What</span>
<span class="sd">                is the capital of France?&quot; will be encoded as &quot;query: What is the capital of France?&quot; because the sentence</span>
<span class="sd">                is appended to the prompt. If ``prompt`` is also set, this argument is ignored. Defaults to None.</span>
<span class="sd">            prompt (Optional[str], optional): The prompt to use for encoding. For example, if the prompt is &quot;query: &quot;, then the</span>
<span class="sd">                sentence &quot;What is the capital of France?&quot; will be encoded as &quot;query: What is the capital of France?&quot;</span>
<span class="sd">                because the sentence is appended to the prompt. If ``prompt`` is set, ``prompt_name`` is ignored. Defaults to None.</span>
<span class="sd">            batch_size (int): Encode sentences with batch size. (default: 32)</span>
<span class="sd">            chunk_size (int): Sentences are chunked and sent to the individual processes. If None, it determines a</span>
<span class="sd">                sensible size. Defaults to None.</span>
<span class="sd">            show_progress_bar (bool, optional): Whether to output a progress bar when encode sentences. Defaults to None.</span>
<span class="sd">            precision (Literal[&quot;float32&quot;, &quot;int8&quot;, &quot;uint8&quot;, &quot;binary&quot;, &quot;ubinary&quot;]): The precision to use for the</span>
<span class="sd">                embeddings. Can be &quot;float32&quot;, &quot;int8&quot;, &quot;uint8&quot;, &quot;binary&quot;, or &quot;ubinary&quot;. All non-float32 precisions</span>
<span class="sd">                are quantized embeddings. Quantized embeddings are smaller in size and faster to compute, but may</span>
<span class="sd">                have lower accuracy. They are useful for reducing the size of the embeddings of a corpus for</span>
<span class="sd">                semantic search, among other tasks. Defaults to &quot;float32&quot;.</span>
<span class="sd">            normalize_embeddings (bool): Whether to normalize returned vectors to have length 1. In that case,</span>
<span class="sd">                the faster dot-product (util.dot_score) instead of cosine similarity can be used. Defaults to False.</span>
<span class="sd">            truncate_dim (int, optional): The dimension to truncate sentence embeddings to.</span>
<span class="sd">                Truncation is especially interesting for `Matryoshka models &lt;https://sbert.net/examples/sentence_transformer/training/matryoshka/README.html&gt;`_,</span>
<span class="sd">                i.e. models that are trained to still produce useful embeddings even if the embedding dimension is reduced.</span>
<span class="sd">                Truncated embeddings require less memory and are faster to perform retrieval with, but note that inference</span>
<span class="sd">                is just as fast, and the embedding performance is worse than the full embeddings. If None, the ``truncate_dim``</span>
<span class="sd">                from the model initialization is used. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            np.ndarray: A 2D numpy array with shape [num_inputs, output_dimension].</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                from sentence_transformers import SentenceTransformer</span>

<span class="sd">                def main():</span>
<span class="sd">                    model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)</span>
<span class="sd">                    sentences = [&quot;The weather is so nice!&quot;, &quot;It&#39;s so sunny outside.&quot;, &quot;He&#39;s driving to the movie theater.&quot;, &quot;She&#39;s going to the cinema.&quot;] * 1000</span>

<span class="sd">                    pool = model.start_multi_process_pool()</span>
<span class="sd">                    embeddings = model.encode_multi_process(sentences, pool)</span>
<span class="sd">                    model.stop_multi_process_pool(pool)</span>

<span class="sd">                    print(embeddings.shape)</span>
<span class="sd">                    # =&gt; (4000, 768)</span>

<span class="sd">                if __name__ == &quot;__main__&quot;:</span>
<span class="sd">                    main()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
            <span class="n">sentences</span><span class="p">,</span>
            <span class="n">prompt_name</span><span class="o">=</span><span class="n">prompt_name</span><span class="p">,</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bar</span><span class="o">=</span><span class="n">show_progress_bar</span><span class="p">,</span>
            <span class="n">output_value</span><span class="o">=</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">convert_to_tensor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">normalize_embeddings</span><span class="o">=</span><span class="n">normalize_embeddings</span><span class="p">,</span>
            <span class="n">truncate_dim</span><span class="o">=</span><span class="n">truncate_dim</span><span class="p">,</span>
            <span class="n">pool</span><span class="o">=</span><span class="n">pool</span><span class="p">,</span>
            <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span>
        <span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_encode_multi_process</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">input_was_string</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pool</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;processes&quot;</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">encode_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">convert_to_tensor</span> <span class="o">=</span> <span class="n">encode_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;convert_to_tensor&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">convert_to_numpy</span> <span class="o">=</span> <span class="n">encode_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;convert_to_numpy&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">encode_kwargs</span><span class="p">[</span><span class="s2">&quot;show_progress_bar&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Create a pool if is not provided, but a list of devices is</span>
        <span class="n">created_pool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_multi_process_pool</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">created_pool</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Determine chunk size if not provided. As a default, aim for 10 chunks per process, with a maximum of 5000 sentences per chunk.</span>
            <span class="k">if</span> <span class="n">chunk_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">chunk_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pool</span><span class="p">[</span><span class="s2">&quot;processes&quot;</span><span class="p">])</span> <span class="o">/</span> <span class="mi">10</span><span class="p">),</span> <span class="mi">5000</span><span class="p">)</span>
                <span class="n">chunk_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Ensure chunk_size is at least 1</span>

            <span class="n">input_queue</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">Queue</span> <span class="o">=</span> <span class="n">pool</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span>
            <span class="n">output_queue</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">Queue</span> <span class="o">=</span> <span class="n">pool</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>

            <span class="c1"># Send inputs to the input queue in chunks</span>
            <span class="n">chunk_id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># We default to -1 to handle empty input gracefully</span>
            <span class="k">for</span> <span class="n">chunk_id</span><span class="p">,</span> <span class="n">chunk_start</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">)):</span>
                <span class="n">chunk</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">chunk_start</span> <span class="p">:</span> <span class="n">chunk_start</span> <span class="o">+</span> <span class="n">chunk_size</span><span class="p">]</span>
                <span class="n">input_queue</span><span class="o">.</span><span class="n">put</span><span class="p">([</span><span class="n">chunk_id</span><span class="p">,</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">encode_kwargs</span><span class="p">])</span>

            <span class="c1"># Collect results from the output queue</span>
            <span class="n">output_list</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
                <span class="p">[</span><span class="n">output_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="n">chunk_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Chunks&quot;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bar</span><span class="p">)],</span>
                <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">input_was_string</span><span class="p">:</span>
                <span class="c1"># If input was a single string, return the first (only) result directly</span>
                <span class="k">return</span> <span class="n">output_list</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Handle the various output formats: torch tensors, numpy arrays, or</span>
            <span class="c1"># list of dictionaries, also when empty.</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">output_list</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">embeddings</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="p">[])</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">convert_to_tensor</span><span class="p">:</span>
                <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
            <span class="k">elif</span> <span class="n">convert_to_numpy</span><span class="p">:</span>
                <span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
            <span class="k">return</span> <span class="n">embeddings</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># Clean up the pool if we created it</span>
            <span class="k">if</span> <span class="n">created_pool</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stop_multi_process_pool</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_encode_multi_process_worker</span><span class="p">(</span>
        <span class="n">target_device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">input_queue</span><span class="p">:</span> <span class="n">Queue</span><span class="p">,</span> <span class="n">results_queue</span><span class="p">:</span> <span class="n">Queue</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Internal working process to encode sentences in multi-process setup</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">chunk_id</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">input_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
                <span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">target_device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="c1"># If multi-process embeddings are not on CPUs, move them to CPU, so they can</span>
                <span class="c1"># all be concatenated later</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">value</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">&quot;cpu&quot;</span> <span class="k">else</span> <span class="n">value</span>
                        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                <span class="n">results_queue</span><span class="o">.</span><span class="n">put</span><span class="p">([</span><span class="n">chunk_id</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">])</span>
            <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
                <span class="k">break</span>

<div class="viewcode-block" id="SentenceTransformer.set_pooling_include_prompt">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.set_pooling_include_prompt">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_pooling_include_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">include_prompt</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the `include_prompt` attribute in the pooling layer in the model, if there is one.</span>

<span class="sd">        This is useful for INSTRUCTOR models, as the prompt should be excluded from the pooling strategy</span>
<span class="sd">        for these models.</span>

<span class="sd">        Args:</span>
<span class="sd">            include_prompt (bool): Whether to include the prompt in the pooling layer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Pooling</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">include_prompt</span> <span class="o">=</span> <span class="n">include_prompt</span>
                <span class="k">break</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_get_prompt_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the length of the prompt in tokens, including the BOS token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">*</span><span class="n">kwargs</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_length_mapping</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_length_mapping</span><span class="p">[(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">*</span><span class="n">kwargs</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

        <span class="n">tokenized_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="n">prompt</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;input_ids&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tokenized_prompt</span><span class="p">:</span>
            <span class="c1"># If the tokenizer does not return input_ids, we cannot determine the prompt length.</span>
            <span class="c1"># This can happen with some tokenizers that do not use input_ids.</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">prompt_length</span> <span class="o">=</span> <span class="n">tokenized_prompt</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># If the tokenizer adds a special EOS token, we do not count it as part of the prompt length.</span>
        <span class="c1"># This is to ensure that the prompt length does not include the EOS token.</span>
        <span class="n">last_token</span> <span class="o">=</span> <span class="n">tokenized_prompt</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;all_special_ids&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">last_token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">all_special_ids</span><span class="p">:</span>
            <span class="n">prompt_length</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prompt_length_mapping</span><span class="p">[(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">*</span><span class="n">kwargs</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span> <span class="o">=</span> <span class="n">prompt_length</span>
        <span class="k">return</span> <span class="n">prompt_length</span>

<div class="viewcode-block" id="SentenceTransformer.get_max_seq_length">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.get_max_seq_length">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_max_seq_length</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the maximal sequence length that the model accepts. Longer inputs will be truncated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Optional[int]: The maximal sequence length that the model accepts, or None if it is not defined.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">(),</span> <span class="s2">&quot;max_seq_length&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">max_seq_length</span>

        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="SentenceTransformer.tokenize">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.tokenize">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenizes the texts.</span>

<span class="sd">        Args:</span>
<span class="sd">            texts (Union[List[str], List[Dict], List[Tuple[str, str]]]): A list of texts to be tokenized.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Tensor]: A dictionary of tensors with the tokenized texts. Common keys are &quot;input_ids&quot;,</span>
<span class="sd">                &quot;attention_mask&quot;, and &quot;token_type_ids&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">get_sentence_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">features</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;sentence_embedding&quot;</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">get_sentence_features</span><span class="p">(</span><span class="o">*</span><span class="n">features</span><span class="p">)</span>

<div class="viewcode-block" id="SentenceTransformer.get_sentence_embedding_dimension">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.get_sentence_embedding_dimension">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_sentence_embedding_dimension</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the number of dimensions in the output of :meth:`SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Optional[int]: The number of dimensions in the output of `encode`. If it&#39;s not known, it&#39;s `None`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_dim</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">mod</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="n">sent_embedding_dim_method</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;get_sentence_embedding_dimension&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">sent_embedding_dim_method</span><span class="p">):</span>
                <span class="n">output_dim</span> <span class="o">=</span> <span class="n">sent_embedding_dim_method</span><span class="p">()</span>
                <span class="k">break</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># The user requested truncation. If they set it to a dim greater than output_dim,</span>
            <span class="c1"># no truncation will actually happen. So return output_dim instead of self.truncate_dim</span>
            <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">output_dim</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_dim</span></div>


<div class="viewcode-block" id="SentenceTransformer.truncate_sentence_embeddings">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.truncate_sentence_embeddings">[docs]</a>
    <span class="nd">@contextmanager</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">truncate_sentence_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">truncate_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="kc">None</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        In this context, :meth:`SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;` outputs</span>
<span class="sd">        sentence embeddings truncated at dimension ``truncate_dim``.</span>

<span class="sd">        This may be useful when you are using the same model for different applications where different dimensions</span>
<span class="sd">        are needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            truncate_dim (int, optional): The dimension to truncate sentence embeddings to. ``None`` does no truncation.</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                from sentence_transformers import SentenceTransformer</span>

<span class="sd">                model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)</span>

<span class="sd">                with model.truncate_sentence_embeddings(truncate_dim=16):</span>
<span class="sd">                    embeddings_truncated = model.encode([&quot;hello there&quot;, &quot;hiya&quot;])</span>
<span class="sd">                assert embeddings_truncated.shape[-1] == 16</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">original_output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_dim</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">truncate_dim</span> <span class="o">=</span> <span class="n">truncate_dim</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">truncate_dim</span> <span class="o">=</span> <span class="n">original_output_dim</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_first_module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the first module of this sequential embedder&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">))]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_last_module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the last module of this sequential embedder&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">next</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">))]</span>

<div class="viewcode-block" id="SentenceTransformer.save">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.save">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">create_model_card</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">train_datasets</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves a model and its configuration files to a directory, so that it can be loaded</span>
<span class="sd">        with ``SentenceTransformer(path)`` again.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str): Path on disk where the model will be saved.</span>
<span class="sd">            model_name (str, optional): Optional model name.</span>
<span class="sd">            create_model_card (bool, optional): If True, create a README.md with basic information about this model.</span>
<span class="sd">            train_datasets (List[str], optional): Optional list with the names of the datasets used to train the model.</span>
<span class="sd">            safe_serialization (bool, optional): If True, save the model using safetensors. If False, save the model</span>
<span class="sd">                the traditional (but unsafe) PyTorch way.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Save model to </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">modules_config</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Save some model info</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="p">[</span><span class="s2">&quot;__version__&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;sentence_transformers&quot;</span><span class="p">:</span> <span class="n">__version__</span><span class="p">,</span>
            <span class="s2">&quot;transformers&quot;</span><span class="p">:</span> <span class="n">transformers</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
            <span class="s2">&quot;pytorch&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;config_sentence_transformers.json&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fOut</span><span class="p">:</span>
            <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;prompts&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;default_prompt_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;similarity_fn_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">similarity_fn_name</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">fOut</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Save modules</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">):</span>
            <span class="n">module</span><span class="p">:</span> <span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;save_in_root&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">save_in_root</span>
            <span class="p">):</span>  <span class="c1"># Save first module in the main folder</span>
                <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Try to save with safetensors, but fall back to the traditional PyTorch way if the module doesn&#39;t support it</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

            <span class="c1"># &quot;module&quot; only works for Sentence Transformers as the modules have the same names as the classes</span>
            <span class="n">class_ref</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__module__</span>
            <span class="c1"># For remote modules, we want to remove &quot;transformers_modules.{repo_name}&quot;:</span>
            <span class="k">if</span> <span class="n">class_ref</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;transformers_modules.&quot;</span><span class="p">):</span>
                <span class="n">class_file</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">class_ref</span><span class="p">]</span><span class="o">.</span><span class="vm">__file__</span>

                <span class="c1"># Save the custom module file</span>
                <span class="n">dest_file</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">class_file</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">shutil</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">class_file</span><span class="p">,</span> <span class="n">dest_file</span><span class="p">)</span>

                <span class="c1"># Save all files importeed in the custom module file</span>
                <span class="k">for</span> <span class="n">needed_file</span> <span class="ow">in</span> <span class="n">get_relative_import_files</span><span class="p">(</span><span class="n">class_file</span><span class="p">):</span>
                    <span class="n">dest_file</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">needed_file</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                    <span class="n">shutil</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">needed_file</span><span class="p">,</span> <span class="n">dest_file</span><span class="p">)</span>

                <span class="c1"># For remote modules, we want to ignore the &quot;transformers_modules.{repo_id}&quot; part,</span>
                <span class="c1"># i.e. we only want the filename</span>
                <span class="n">class_ref</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_ref</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="c1"># For other cases, we want to add the class name:</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">class_ref</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;sentence_transformers.&quot;</span><span class="p">):</span>
                <span class="n">class_ref</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_ref</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="n">module_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="n">idx</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">model_path</span><span class="p">),</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">class_ref</span><span class="p">}</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span> <span class="ow">and</span> <span class="p">(</span><span class="n">module_kwargs</span> <span class="o">:=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_kwargs</span><span class="p">[</span><span class="n">name</span><span class="p">]):</span>
                <span class="n">module_config</span><span class="p">[</span><span class="s2">&quot;kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_kwargs</span>
            <span class="n">modules_config</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module_config</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;modules.json&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fOut</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">modules_config</span><span class="p">,</span> <span class="n">fOut</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Create model card</span>
        <span class="k">if</span> <span class="n">create_model_card</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_create_model_card</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">train_datasets</span><span class="p">)</span></div>


<div class="viewcode-block" id="SentenceTransformer.save_pretrained">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.save_pretrained">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_pretrained</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">create_model_card</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">train_datasets</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves a model and its configuration files to a directory, so that it can be loaded</span>
<span class="sd">        with ``SentenceTransformer(path)`` again.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str): Path on disk where the model will be saved.</span>
<span class="sd">            model_name (str, optional): Optional model name.</span>
<span class="sd">            create_model_card (bool, optional): If True, create a README.md with basic information about this model.</span>
<span class="sd">            train_datasets (List[str], optional): Optional list with the names of the datasets used to train the model.</span>
<span class="sd">            safe_serialization (bool, optional): If True, save the model using safetensors. If False, save the model</span>
<span class="sd">                the traditional (but unsafe) PyTorch way.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
            <span class="n">path</span><span class="p">,</span>
            <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">create_model_card</span><span class="o">=</span><span class="n">create_model_card</span><span class="p">,</span>
            <span class="n">train_datasets</span><span class="o">=</span><span class="n">train_datasets</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_update_default_model_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_card</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">model_id</span><span class="p">:</span>
            <span class="n">model_card</span> <span class="o">=</span> <span class="n">model_card</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
                <span class="s1">&#39;model = SentenceTransformer(&quot;sentence_transformers_model_id&quot;&#39;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s1">&#39;model = SentenceTransformer(&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">model_id</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">model_card</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_model_card</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">train_datasets</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="s2">&quot;deprecated&quot;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create an automatic model and stores it in the specified path. If no training was done and the loaded model</span>
<span class="sd">        was a Sentence Transformer model already, then its model card is reused.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str): The path where the model card will be stored.</span>
<span class="sd">            model_name (Optional[str], optional): The name of the model. Defaults to None.</span>
<span class="sd">            train_datasets (Optional[List[str]], optional): Deprecated argument. Defaults to &quot;deprecated&quot;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">model_name</span><span class="p">:</span>
            <span class="n">model_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">model_path</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">model_id</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">model_id</span> <span class="o">=</span> <span class="n">model_name</span>

        <span class="c1"># If we loaded a Sentence Transformer model from the Hub, and no training was done, then</span>
        <span class="c1"># we don&#39;t generate a new model card, but reuse the old one instead.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_card_text</span> <span class="ow">and</span> <span class="s2">&quot;generated_from_trainer&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">tags</span><span class="p">:</span>
            <span class="n">model_card</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_card_text</span>
            <span class="n">model_card</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_default_model_id</span><span class="p">(</span><span class="n">model_card</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">model_card</span> <span class="o">=</span> <span class="n">generate_model_card</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Error while generating model card:</span><span class="se">\n</span><span class="si">{</span><span class="n">traceback</span><span class="o">.</span><span class="n">format_exc</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;Consider opening an issue on https://github.com/huggingface/sentence-transformers/issues with this traceback.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;Skipping model card creation.&quot;</span>
                <span class="p">)</span>
                <span class="k">return</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;README.md&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fOut</span><span class="p">:</span>
            <span class="n">fOut</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">model_card</span><span class="p">)</span>

<div class="viewcode-block" id="SentenceTransformer.save_to_hub">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.save_to_hub">[docs]</a>
    <span class="nd">@save_to_hub_args_decorator</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_to_hub</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">organization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">private</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">commit_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Add new SentenceTransformer model.&quot;</span><span class="p">,</span>
        <span class="n">local_model_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">exist_ok</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">replace_model_card</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">train_datasets</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        DEPRECATED, use `push_to_hub` instead.</span>

<span class="sd">        Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.</span>

<span class="sd">        Args:</span>
<span class="sd">            repo_id (str): Repository name for your model in the Hub, including the user or organization.</span>
<span class="sd">            token (str, optional): An authentication token (See https://huggingface.co/settings/token)</span>
<span class="sd">            private (bool, optional): Set to true, for hosting a private model</span>
<span class="sd">            safe_serialization (bool, optional): If true, save the model using safetensors. If false, save the model the traditional PyTorch way</span>
<span class="sd">            commit_message (str, optional): Message to commit while pushing.</span>
<span class="sd">            local_model_path (str, optional): Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded</span>
<span class="sd">            exist_ok (bool, optional): If true, saving to an existing repository is OK. If false, saving only to a new repository is possible</span>
<span class="sd">            replace_model_card (bool, optional): If true, replace an existing model card in the hub with the automatically created model card</span>
<span class="sd">            train_datasets (List[str], optional): Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The url of the commit of your model in the repository on the Hugging Face Hub.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;The `save_to_hub` method is deprecated and will be removed in a future version of SentenceTransformers.&quot;</span>
            <span class="s2">&quot; Please use `push_to_hub` instead for future model uploads.&quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">organization</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;/&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">repo_id</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;Providing an `organization` to `save_to_hub` is deprecated, please use `repo_id=&quot;</span><span class="si">{</span><span class="n">organization</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s1">&quot;` instead.&#39;</span>
                <span class="p">)</span>
                <span class="n">repo_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">organization</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">elif</span> <span class="n">repo_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">organization</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Providing an `organization` to `save_to_hub` is deprecated, please only use `repo_id`.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;Providing an `organization` to `save_to_hub` is deprecated, please only use `repo_id=&quot;</span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s1">&quot;` instead.&#39;</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>
            <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">private</span><span class="o">=</span><span class="n">private</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">,</span>
            <span class="n">local_model_path</span><span class="o">=</span><span class="n">local_model_path</span><span class="p">,</span>
            <span class="n">exist_ok</span><span class="o">=</span><span class="n">exist_ok</span><span class="p">,</span>
            <span class="n">replace_model_card</span><span class="o">=</span><span class="n">replace_model_card</span><span class="p">,</span>
            <span class="n">train_datasets</span><span class="o">=</span><span class="n">train_datasets</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="SentenceTransformer.push_to_hub">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.push_to_hub">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">push_to_hub</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">private</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">commit_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_model_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">exist_ok</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">replace_model_card</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">train_datasets</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">create_pr</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.</span>

<span class="sd">        Args:</span>
<span class="sd">            repo_id (str): Repository name for your model in the Hub, including the user or organization.</span>
<span class="sd">            token (str, optional): An authentication token (See https://huggingface.co/settings/token)</span>
<span class="sd">            private (bool, optional): Set to true, for hosting a private model</span>
<span class="sd">            safe_serialization (bool, optional): If true, save the model using safetensors. If false, save the model the traditional PyTorch way</span>
<span class="sd">            commit_message (str, optional): Message to commit while pushing.</span>
<span class="sd">            local_model_path (str, optional): Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded</span>
<span class="sd">            exist_ok (bool, optional): If true, saving to an existing repository is OK. If false, saving only to a new repository is possible</span>
<span class="sd">            replace_model_card (bool, optional): If true, replace an existing model card in the hub with the automatically created model card</span>
<span class="sd">            train_datasets (List[str], optional): Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.</span>
<span class="sd">            revision (str, optional): Branch to push the uploaded files to</span>
<span class="sd">            create_pr (bool, optional): If True, create a pull request instead of pushing directly to the main branch</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The url of the commit of your model in the repository on the Hugging Face Hub.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">api</span> <span class="o">=</span> <span class="n">HfApi</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">)</span>
        <span class="n">repo_url</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">create_repo</span><span class="p">(</span>
            <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
            <span class="n">private</span><span class="o">=</span><span class="n">private</span><span class="p">,</span>
            <span class="n">repo_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">exist_ok</span><span class="o">=</span><span class="n">exist_ok</span> <span class="ow">or</span> <span class="n">create_pr</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">repo_id</span> <span class="o">=</span> <span class="n">repo_url</span><span class="o">.</span><span class="n">repo_id</span>  <span class="c1"># Update the repo_id in case the old repo_id didn&#39;t contain a user or organization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">set_model_id</span><span class="p">(</span><span class="n">repo_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">revision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">api</span><span class="o">.</span><span class="n">create_branch</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">branch</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">commit_message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">backend</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
                <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Add new </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> model&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Add new </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> model with an </span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2"> backend&quot;</span>

        <span class="n">commit_description</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">if</span> <span class="n">create_pr</span><span class="p">:</span>
            <span class="n">commit_description</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Hello!</span>

<span class="s2">*This pull request has been automatically generated from the [`push_to_hub`](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.push_to_hub) method from the Sentence Transformers library.*</span>

<span class="s2">## Full Model Architecture:</span>
<span class="s2">```</span>
<span class="si">{</span><span class="bp">self</span><span class="si">}</span>
<span class="s2">```</span>

<span class="s2">## Tip:</span>
<span class="s2">Consider testing this pull request before merging by loading the model from this PR with the `revision` argument:</span>
<span class="s2">```python</span>
<span class="s2">from sentence_transformers import </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span>

<span class="s2"># TODO: Fill in the PR number</span>
<span class="s2">pr_number = 2</span>
<span class="s2">model = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span>
<span class="s2">    &quot;</span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s2">&quot;,</span>
<span class="s2">    revision=f&quot;refs/pr/</span><span class="se">{{</span><span class="s2">pr_number</span><span class="se">}}</span><span class="s2">&quot;,</span>
<span class="s2">    backend=&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;,</span>
<span class="s2">)</span>

<span class="s2"># Verify that everything works as expected</span>
<span class="s2">embeddings = model.encode([&quot;The weather is lovely today.&quot;, &quot;It&#39;s so sunny outside!&quot;, &quot;He drove to the stadium.&quot;])</span>
<span class="s2">print(embeddings.shape)</span>

<span class="s2">similarities = model.similarity(embeddings, embeddings)</span>
<span class="s2">print(similarities)</span>
<span class="s2">```</span>
<span class="s2">&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">local_model_path</span><span class="p">:</span>
            <span class="n">folder_url</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">upload_folder</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
                <span class="n">folder_path</span><span class="o">=</span><span class="n">local_model_path</span><span class="p">,</span>
                <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">,</span>
                <span class="n">commit_description</span><span class="o">=</span><span class="n">commit_description</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">create_pr</span><span class="o">=</span><span class="n">create_pr</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmp_dir</span><span class="p">:</span>
                <span class="n">create_model_card</span> <span class="o">=</span> <span class="n">replace_model_card</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmp_dir</span><span class="p">,</span> <span class="s2">&quot;README.md&quot;</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span>
                    <span class="n">tmp_dir</span><span class="p">,</span>
                    <span class="n">model_name</span><span class="o">=</span><span class="n">repo_url</span><span class="o">.</span><span class="n">repo_id</span><span class="p">,</span>
                    <span class="n">create_model_card</span><span class="o">=</span><span class="n">create_model_card</span><span class="p">,</span>
                    <span class="n">train_datasets</span><span class="o">=</span><span class="n">train_datasets</span><span class="p">,</span>
                    <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">folder_url</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">upload_folder</span><span class="p">(</span>
                    <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
                    <span class="n">folder_path</span><span class="o">=</span><span class="n">tmp_dir</span><span class="p">,</span>
                    <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">,</span>
                    <span class="n">commit_description</span><span class="o">=</span><span class="n">commit_description</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">create_pr</span><span class="o">=</span><span class="n">create_pr</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">create_pr</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">folder_url</span><span class="o">.</span><span class="n">pr_url</span>
        <span class="k">return</span> <span class="n">folder_url</span><span class="o">.</span><span class="n">commit_url</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_text_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Help function to get the length for the input text. Text can be either</span>
<span class="sd">        a list of ints (which means a single text as input), or a tuple of list of ints</span>
<span class="sd">        (representing several text inputs to the model).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>  <span class="c1"># {key: value} case</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">values</span><span class="p">())))</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="s2">&quot;__len__&quot;</span><span class="p">):</span>  <span class="c1"># Object has no len() method</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>  <span class="c1"># Empty string or list of ints</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">text</span><span class="p">])</span>  <span class="c1"># Sum of length of individual strings</span>

<div class="viewcode-block" id="SentenceTransformer.evaluate">
<a class="viewcode-back" href="../../api/movie_recommender.html#netflix_recommender.movie_recommender.SentenceTransformer.evaluate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">evaluator</span><span class="p">:</span> <span class="n">SentenceEvaluator</span><span class="p">,</span> <span class="n">output_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the model based on an evaluator</span>

<span class="sd">        Args:</span>
<span class="sd">            evaluator (SentenceEvaluator): The evaluator used to evaluate the model.</span>
<span class="sd">            output_path (str, optional): The path where the evaluator can write the results. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The evaluation results.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">output_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">evaluator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_path</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_load_auto_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trust_remote_code</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">config_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">has_modules</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a simple Transformer + Mean Pooling model and returns the modules</span>

<span class="sd">        Args:</span>
<span class="sd">            model_name_or_path (str): The name or path of the pre-trained model.</span>
<span class="sd">            token (Optional[Union[bool, str]]): The token to use for the model.</span>
<span class="sd">            cache_folder (Optional[str]): The folder to cache the model.</span>
<span class="sd">            revision (Optional[str], optional): The revision of the model. Defaults to None.</span>
<span class="sd">            trust_remote_code (bool, optional): Whether to trust remote code. Defaults to False.</span>
<span class="sd">            local_files_only (bool, optional): Whether to use only local files. Defaults to False.</span>
<span class="sd">            model_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments for the model. Defaults to None.</span>
<span class="sd">            tokenizer_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments for the tokenizer. Defaults to None.</span>
<span class="sd">            config_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments for the config. Defaults to None.</span>
<span class="sd">            has_modules (bool, optional): Whether the model has modules.json. Defaults to False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[nn.Module]: A list containing the transformer model and the pooling model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;No sentence-transformers model found with name </span><span class="si">{</span><span class="n">model_name_or_path</span><span class="si">}</span><span class="s2">. Creating a new one with mean pooling.&quot;</span>
        <span class="p">)</span>

        <span class="n">shared_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;token&quot;</span><span class="p">:</span> <span class="n">token</span><span class="p">,</span>
            <span class="s2">&quot;trust_remote_code&quot;</span><span class="p">:</span> <span class="n">trust_remote_code</span><span class="p">,</span>
            <span class="s2">&quot;revision&quot;</span><span class="p">:</span> <span class="n">revision</span><span class="p">,</span>
            <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">shared_kwargs</span> <span class="k">if</span> <span class="n">model_kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{</span><span class="o">**</span><span class="n">shared_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">}</span>
        <span class="n">tokenizer_kwargs</span> <span class="o">=</span> <span class="n">shared_kwargs</span> <span class="k">if</span> <span class="n">tokenizer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{</span><span class="o">**</span><span class="n">shared_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">tokenizer_kwargs</span><span class="p">}</span>
        <span class="n">config_kwargs</span> <span class="o">=</span> <span class="n">shared_kwargs</span> <span class="k">if</span> <span class="n">config_kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{</span><span class="o">**</span><span class="n">shared_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">config_kwargs</span><span class="p">}</span>

        <span class="n">transformer_model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
            <span class="n">model_args</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="n">tokenizer_args</span><span class="o">=</span><span class="n">tokenizer_kwargs</span><span class="p">,</span>
            <span class="n">config_args</span><span class="o">=</span><span class="n">config_kwargs</span><span class="p">,</span>
            <span class="n">backend</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">pooling_model</span> <span class="o">=</span> <span class="n">Pooling</span><span class="p">(</span><span class="n">transformer_model</span><span class="o">.</span><span class="n">get_word_embedding_dimension</span><span class="p">(),</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">local_files_only</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">set_base_model</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">transformer_model</span><span class="p">,</span> <span class="n">pooling_model</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_load_module_class_from_ref</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">class_ref</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">trust_remote_code</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="c1"># If the class is from sentence_transformers, we can directly import it,</span>
        <span class="c1"># otherwise, we try to import it dynamically, and if that fails, we fall back to the default import</span>
        <span class="k">if</span> <span class="n">class_ref</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;sentence_transformers.&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">import_from_string</span><span class="p">(</span><span class="n">class_ref</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">trust_remote_code</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">):</span>
            <span class="n">code_revision</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;code_revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">if</span> <span class="n">model_kwargs</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">get_class_from_dynamic_module</span><span class="p">(</span>
                    <span class="n">class_ref</span><span class="p">,</span>
                    <span class="n">model_name_or_path</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">code_revision</span><span class="o">=</span><span class="n">code_revision</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">OSError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
                <span class="c1"># Ignore the error if 1) the file does not exist, or 2) the class_ref is not correctly formatted/found</span>
                <span class="k">pass</span>

        <span class="k">return</span> <span class="n">import_from_string</span><span class="p">(</span><span class="n">class_ref</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_load_sbert_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trust_remote_code</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">config_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads a full SentenceTransformer model using the modules.json file.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_name_or_path (str): The name or path of the pre-trained model.</span>
<span class="sd">            token (Optional[Union[bool, str]]): The token to use for the model.</span>
<span class="sd">            cache_folder (Optional[str]): The folder to cache the model.</span>
<span class="sd">            revision (Optional[str], optional): The revision of the model. Defaults to None.</span>
<span class="sd">            trust_remote_code (bool, optional): Whether to trust remote code. Defaults to False.</span>
<span class="sd">            local_files_only (bool, optional): Whether to use only local files. Defaults to False.</span>
<span class="sd">            model_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments for the model. Defaults to None.</span>
<span class="sd">            tokenizer_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments for the tokenizer. Defaults to None.</span>
<span class="sd">            config_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments for the config. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            OrderedDict[str, nn.Module]: An ordered dictionary containing the modules of the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check if the config_sentence_transformers.json file exists (exists since v2 of the framework)</span>
        <span class="n">config_sentence_transformers_json_path</span> <span class="o">=</span> <span class="n">load_file_path</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span>
            <span class="s2">&quot;config_sentence_transformers.json&quot;</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">config_sentence_transformers_json_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_sentence_transformers_json_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fIn</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fIn</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;__version__&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span>
                <span class="ow">and</span> <span class="s2">&quot;sentence_transformers&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="p">[</span><span class="s2">&quot;__version__&quot;</span><span class="p">]</span>
                <span class="ow">and</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="p">[</span><span class="s2">&quot;__version__&quot;</span><span class="p">][</span><span class="s2">&quot;sentence_transformers&quot;</span><span class="p">])</span>
                <span class="o">&gt;</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">__version__</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;You are trying to use a model that was created with Sentence Transformers version </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="p">[</span><span class="s1">&#39;__version__&#39;</span><span class="p">][</span><span class="s1">&#39;sentence_transformers&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but you&#39;re currently using version </span><span class="si">{</span><span class="n">__version__</span><span class="si">}</span><span class="s2">. This might cause unexpected behavior or errors. &quot;</span>
                    <span class="s2">&quot;In that case, try to update to the latest version.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Set score functions &amp; prompts if not already overridden by the __init__ calls</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_similarity_fn_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">similarity_fn_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;similarity_fn_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="c1"># Only update prompts that aren&#39;t already set by the user or defaults</span>
            <span class="k">for</span> <span class="n">prompt_name</span><span class="p">,</span> <span class="n">prompt_text</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;prompts&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">prompt_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="p">[</span><span class="n">prompt_name</span><span class="p">]:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="p">[</span><span class="n">prompt_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">prompt_text</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">default_prompt_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;default_prompt_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;model_type&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="c1"># Check if a readme exists</span>
        <span class="n">model_card_path</span> <span class="o">=</span> <span class="n">load_file_path</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span>
            <span class="s2">&quot;README.md&quot;</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">model_card_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_card_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fIn</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_model_card_text</span> <span class="o">=</span> <span class="n">fIn</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="c1"># Load the modules of sentence transformer</span>
        <span class="n">modules_json_path</span> <span class="o">=</span> <span class="n">load_file_path</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span>
            <span class="s2">&quot;modules.json&quot;</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">modules_json_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fIn</span><span class="p">:</span>
            <span class="n">modules_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fIn</span><span class="p">)</span>

        <span class="n">modules</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">module_kwargs</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">module_config</span> <span class="ow">in</span> <span class="n">modules_config</span><span class="p">:</span>
            <span class="n">class_ref</span> <span class="o">=</span> <span class="n">module_config</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span>
            <span class="n">module_class</span><span class="p">:</span> <span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_module_class_from_ref</span><span class="p">(</span>
                <span class="n">class_ref</span><span class="p">,</span> <span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="p">,</span> <span class="n">revision</span><span class="p">,</span> <span class="n">model_kwargs</span>
            <span class="p">)</span>

            <span class="c1"># Backwards compatibility: if the module is older and its `load` method only supports one parameter,</span>
            <span class="c1"># a path to a local directory containing the module files, then we load it with the old style</span>
            <span class="n">load_signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">module_class</span><span class="o">.</span><span class="n">load</span><span class="p">)</span>
            <span class="c1"># Check if the `load` method only accepts a single parameter (the path to the local directory).</span>
            <span class="c1"># This indicates an older module that does not support the newer loading method with multiple arguments.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">load_signature</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">module_class</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>
                <span class="c1"># If the module&#39;s `__init__` method contains specific keyword arguments like `model_args` and `config_args`,</span>
                <span class="c1"># it is likely Transformer-based. These arguments are commonly used in Transformer models to configure</span>
                <span class="c1"># the model and tokenizer during initialization.</span>
                <span class="c1"># Example: Models with custom modules on the Hugging Face Hub like</span>
                <span class="c1"># https://huggingface.co/jinaai/jina-embeddings-v3 may use this logic.</span>
                <span class="k">if</span> <span class="p">{</span><span class="s2">&quot;model_args&quot;</span><span class="p">,</span> <span class="s2">&quot;config_args&quot;</span><span class="p">}</span> <span class="o">&lt;=</span> <span class="nb">set</span><span class="p">(</span><span class="n">signature</span><span class="o">.</span><span class="n">parameters</span><span class="p">):</span>
                    <span class="c1"># Load initialization arguments specific to Transformer-based modules. This includes</span>
                    <span class="c1"># arguments for loading the model, tokenizer, and configuration, as well as any</span>
                    <span class="c1"># additional module-specific keyword arguments.</span>
                    <span class="n">common_transformer_init_kwargs</span> <span class="o">=</span> <span class="n">Transformer</span><span class="o">.</span><span class="n">_load_init_kwargs</span><span class="p">(</span>
                        <span class="n">model_name_or_path</span><span class="p">,</span>
                        <span class="c1"># Loading-specific keyword arguments</span>
                        <span class="n">subfolder</span><span class="o">=</span><span class="n">module_config</span><span class="p">[</span><span class="s2">&quot;path&quot;</span><span class="p">],</span>
                        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                        <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
                        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                        <span class="c1"># Module-specific keyword arguments</span>
                        <span class="n">trust_remote_code</span><span class="o">=</span><span class="n">trust_remote_code</span><span class="p">,</span>
                        <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
                        <span class="n">tokenizer_kwargs</span><span class="o">=</span><span class="n">tokenizer_kwargs</span><span class="p">,</span>
                        <span class="n">config_kwargs</span><span class="o">=</span><span class="n">config_kwargs</span><span class="p">,</span>
                        <span class="n">backend</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="n">module_class</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">common_transformer_init_kwargs</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Old modules that don&#39;t support the new loading method and don&#39;t seem Transformer-based</span>
                    <span class="c1"># are loaded by downloading the full directories and calling .load() with the old style</span>
                    <span class="c1"># (i.e. only a path to the local directory)</span>
                    <span class="n">local_path</span> <span class="o">=</span> <span class="n">load_dir_path</span><span class="p">(</span>
                        <span class="n">model_name_or_path</span><span class="o">=</span><span class="n">model_name_or_path</span><span class="p">,</span>
                        <span class="n">subfolder</span><span class="o">=</span><span class="n">module_config</span><span class="p">[</span><span class="s2">&quot;path&quot;</span><span class="p">],</span>
                        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                        <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
                        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="n">module_class</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">local_path</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Newer modules that support the new loading method are loaded with the new style</span>
                <span class="c1"># i.e. with many keyword arguments that can optionally be used by the modules</span>
                <span class="n">module</span> <span class="o">=</span> <span class="n">module_class</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                    <span class="n">model_name_or_path</span><span class="p">,</span>
                    <span class="c1"># Loading-specific keyword arguments</span>
                    <span class="n">subfolder</span><span class="o">=</span><span class="n">module_config</span><span class="p">[</span><span class="s2">&quot;path&quot;</span><span class="p">],</span>
                    <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                    <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="c1"># Module-specific keyword arguments</span>
                    <span class="n">trust_remote_code</span><span class="o">=</span><span class="n">trust_remote_code</span><span class="p">,</span>
                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
                    <span class="n">tokenizer_kwargs</span><span class="o">=</span><span class="n">tokenizer_kwargs</span><span class="p">,</span>
                    <span class="n">config_kwargs</span><span class="o">=</span><span class="n">config_kwargs</span><span class="p">,</span>
                    <span class="n">backend</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">modules</span><span class="p">[</span><span class="n">module_config</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">module</span>
            <span class="n">module_kwargs</span><span class="p">[</span><span class="n">module_config</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">module_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;kwargs&quot;</span><span class="p">,</span> <span class="p">[])</span>

        <span class="k">if</span> <span class="n">revision</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">path_parts</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">modules_json_path</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">path_parts</span><span class="o">.</span><span class="n">parts</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">revision_path_part</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">modules_json_path</span><span class="p">)</span><span class="o">.</span><span class="n">parts</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">revision_path_part</span><span class="p">)</span> <span class="o">==</span> <span class="mi">40</span><span class="p">:</span>
                    <span class="n">revision</span> <span class="o">=</span> <span class="n">revision_path_part</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">local_files_only</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_card_data</span><span class="o">.</span><span class="n">set_base_model</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">modules</span><span class="p">,</span> <span class="n">module_kwargs</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;SentenceTransformer.load(...) is deprecated, use SentenceTransformer(...) instead.&quot;</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="n">input_path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SentenceTransformer</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">input_path</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">device</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get torch.device from module, assuming that the whole module has one device.</span>
<span class="sd">        In case there are no PyTorch parameters, fall back to CPU.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">transformers_model</span> <span class="o">:=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformers_model</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">transformers_model</span><span class="o">.</span><span class="n">device</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;auto_model&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">auto_model</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">auto_model</span><span class="o">.</span><span class="n">device</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="c1"># For nn.DataParallel compatibility in PyTorch 1.5</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">find_tensor_attributes</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
                <span class="n">tuples</span> <span class="o">=</span> <span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)]</span>
                <span class="k">return</span> <span class="n">tuples</span>

            <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span><span class="n">get_members_fn</span><span class="o">=</span><span class="n">find_tensor_attributes</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">first_tuple</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">first_tuple</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
            <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property to get the tokenizer that is used by this model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">tokenizer</span>

    <span class="nd">@tokenizer</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property to set the tokenizer that should be used by this model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">max_seq_length</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the maximal input sequence length for the model. Longer inputs will be truncated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The maximal input sequence length.</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                from sentence_transformers import SentenceTransformer</span>

<span class="sd">                model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)</span>
<span class="sd">                print(model.max_seq_length)</span>
<span class="sd">                # =&gt; 384</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">max_seq_length</span>

    <span class="nd">@max_seq_length</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">max_seq_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property to set the maximal input sequence length for the model. Longer inputs will be truncated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">transformers_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PreTrainedModel</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property to get the underlying transformers PreTrainedModel instance, if it exists.</span>
<span class="sd">        Note that it&#39;s possible for a model to have multiple underlying transformers models, but this property</span>
<span class="sd">        will return the first one it finds in the module hierarchy.</span>

<span class="sd">        Returns:</span>
<span class="sd">            PreTrainedModel or None: The underlying transformers model or None if not found.</span>

<span class="sd">        Example:</span>
<span class="sd">            ::</span>

<span class="sd">                from sentence_transformers import SentenceTransformer</span>

<span class="sd">                model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)</span>

<span class="sd">                # You can now access the underlying transformers model</span>
<span class="sd">                transformers_model = model.transformers_model</span>
<span class="sd">                print(type(transformers_model))</span>
<span class="sd">                # =&gt; &lt;class &#39;transformers.models.mpnet.modeling_mpnet.MPNetModel&#39;&gt;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">PreTrainedModel</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">module</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_target_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>

    <span class="nd">@_target_device</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_target_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">child</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_no_split_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">_no_split_modules</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_keys_to_ignore_on_save</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">gradient_checkpointing_enable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient_checkpointing_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Propagate the gradient checkpointing to the transformer model</span>
        <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">PreTrainedModel</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">child</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">(</span><span class="n">gradient_checkpointing_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_model_type</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieves the model_type from the config_sentence_transformers.json file.</span>

<span class="sd">        This is used to determine the appropriate loading method:</span>
<span class="sd">        - SentenceTransformer: These models should be loaded with _load_sbert_model when used with SentenceTransformer class</span>
<span class="sd">        - SparseEncoder: These models should be loaded with _load_auto_model when used with SentenceTransformer class</span>

<span class="sd">        When a model type doesn&#39;t match the class being used to load it, we switch loading methods</span>
<span class="sd">        to ensure compatibility.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_name_or_path (str): The name or path of the pre-trained model.</span>
<span class="sd">            token (Optional[Union[bool, str]]): The token to use for the model.</span>
<span class="sd">            cache_folder (Optional[str]): The folder to cache the model.</span>
<span class="sd">            revision (Optional[str], optional): The revision of the model. Defaults to None.</span>
<span class="sd">            local_files_only (bool, optional): Whether to use only local files. Defaults to False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Optional[str]: The model type (SentenceTransformer or SparseEncoder) if available, None otherwise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config_sentence_transformers_json_path</span> <span class="o">=</span> <span class="n">load_file_path</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span>
            <span class="s2">&quot;config_sentence_transformers.json&quot;</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">cache_folder</span><span class="o">=</span><span class="n">cache_folder</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">config_sentence_transformers_json_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;SentenceTransformer&quot;</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_sentence_transformers_json_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fIn</span><span class="p">:</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fIn</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_type&quot;</span><span class="p">,</span> <span class="s2">&quot;SentenceTransformer&quot;</span><span class="p">)</span>  <span class="c1"># Default to &quot;SentenceTransformer&quot; if not specified</span></div>

</pre></div>

                </article>
              
              
              
              
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 9.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>